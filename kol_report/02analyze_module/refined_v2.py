import os
import json
import sqlite3
import requests
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import base64
from io import BytesIO
import argparse
import logging
import time
import re
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from openai import OpenAI
from openai import AsyncOpenAI
import math
import random
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
import sys
import io

# å¼ºåˆ¶æ ‡å‡†è¾“å‡ºä¸ºutf-8
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from prompts import get_prompt_template
from html_generator import HTMLReportGenerator

# ä¿®å¤å¯¼å…¥çš„ç±»å
try:
    from paste_2 import CompleteTechnicalAnalyzer as ExternalTechnicalAgent
    EXTERNAL_AGENT_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥å¤–éƒ¨CompleteTechnicalAgent")
except ImportError:
    EXTERNAL_AGENT_AVAILABLE = False
    print("âš ï¸ æœªæ‰¾åˆ°paste_2.pyï¼Œä½¿ç”¨å†…ç½®CompleteTechnicalAgentEnhanced")

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
try:
    # ä½¿ç”¨ä½ ç³»ç»Ÿä¸Šå®é™…å­˜åœ¨çš„ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = [
        'Noto Serif CJK SC', 'Noto Serif CJK TC', 'Noto Serif CJK JP', 'Noto Serif CJK KR',
        'Noto Serif CJK SC Regular', 'Noto Serif CJK SC Medium', 'Noto Serif CJK SC Bold',
        'Liberation Sans', 'DejaVu Sans', 'Arial'
    ]
    plt.rcParams['axes.unicode_minus'] = False
    
    # éªŒè¯å­—ä½“æ˜¯å¦å¯ç”¨
    import matplotlib.font_manager as fm
    test_font = fm.findfont('Noto Serif CJK SC')
    print(f"âœ… ä½¿ç”¨å­—ä½“: {test_font}")
    
except Exception as e:
    print(f"âš ï¸ å­—ä½“è®¾ç½®å¤±è´¥: {e}")
    # å¤‡ç”¨å­—ä½“è®¾ç½®
    plt.rcParams['font.sans-serif'] = ['Liberation Sans', 'DejaVu Sans', 'Arial']
    plt.rcParams['axes.unicode_minus'] = False

# é…ç½®
OPENAI_MODEL = "o3-2025-04-16"
MINI_MODEL = "o3-2025-04-16"
VISION_MODEL = "o3-2025-04-16"
SEARCH_MODEL = "gpt-4o-search-preview-2025-03-11"
MAX_REQUESTS_PER_MINUTE = 60

# æ’é™¤çš„èµ„äº§ç±»å‹
EXCLUDED_ASSETS = [
    'USD', 'EUR', 'CNY', 'JPY', 'GBP', 'CAD', 'AUD', 'CHF', 'KRW',
    'USDT', 'USDC', 'BUSD', 'DAI', 'FRAX', 'TUSD', 'LUSD',
    'AAPL', 'TSLA', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA',
    'DXY', 'USDCNY', 'EURUSD', 'GBPUSD'
]

# ========================================================================
# çœŸå®æ—¶é—´æˆ³éªŒè¯å¼•æ“ - ä¿®å¤ç‰ˆï¼Œè¡¥å……æ‰€æœ‰ç¼ºå¤±æ–¹æ³•
# ========================================================================

class RealTimestampVerificationEngine:
    """çœŸå®æ—¶é—´æˆ³éªŒè¯å¼•æ“ - ä¿®å¤ç‰ˆï¼Œå¢å¼ºé”™è¯¯å¤„ç†å’Œè¡¥å……ç¼ºå¤±æ–¹æ³•"""

    def __init__(self, coingecko_api_key: Optional[str] = None):
        self.coingecko_api_key = coingecko_api_key

        if coingecko_api_key:
            self.base_url = "https://pro-api.coingecko.com/api/v3"
            self.headers = {"x-cg-pro-api-key": coingecko_api_key}
            self.rate_limit = 0.12  # 500/min for pro
        else:
            self.base_url = "https://api.coingecko.com/api/v3"
            self.headers = {}
            self.rate_limit = 2.5   # 25/min for free

        self.logger = logging.getLogger("RealTimestampVerificationEngine")

        # è®¾ç½®sessionå’Œé‡è¯•ç­–ç•¥
        self.session = requests.Session()
        retry_strategy = Retry(
            total=5,
            backoff_factor=2,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # æ— æ•ˆå¸ç§IDé»‘åå•
        self.invalid_coin_ids = {
            'xxx_kaito', 'xxx_bitcoin', 'test_btc', 'sample_eth',
            'mock_sol', 'demo_pendle', 'fake_aave', 'invalid_coin'
        }

    def validate_api_parameters(self, coin_id: str, timestamp: int) -> bool:
        """éªŒè¯APIå‚æ•° - è¡¥å……ç¼ºå¤±æ–¹æ³•"""
        try:
            # éªŒè¯å¸ç§ID
            if not coin_id or not isinstance(coin_id, str):
                self.logger.error(f"æ— æ•ˆçš„å¸ç§ID: {coin_id}")
                return False
            
            coin_id = coin_id.lower().strip()
            
            # æ£€æŸ¥é»‘åå•
            if coin_id in self.invalid_coin_ids:
                self.logger.warning(f"å¸ç§IDåœ¨é»‘åå•ä¸­: {coin_id}")
                return False
            
            # æ£€æŸ¥æ˜æ˜¾æ— æ•ˆçš„æ ¼å¼
            if coin_id.startswith(('xxx_', 'test_', 'sample_', 'mock_', 'demo_', 'fake_')):
                self.logger.warning(f"å¸ç§IDæ ¼å¼æ— æ•ˆ: {coin_id}")
                return False
            
            # åŸºæœ¬é•¿åº¦å’Œæ ¼å¼æ£€æŸ¥
            if len(coin_id) < 2 or len(coin_id) > 50:
                self.logger.warning(f"å¸ç§IDé•¿åº¦æ— æ•ˆ: {coin_id}")
                return False
            
            # éªŒè¯æ—¶é—´æˆ³
            if not isinstance(timestamp, int):
                self.logger.error(f"æ—¶é—´æˆ³å¿…é¡»æ˜¯æ•´æ•°: {timestamp}")
                return False
            
            # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åœ¨åˆç†èŒƒå›´å†… (2010-2030)
            min_ts = int(datetime(2010, 1, 1).timestamp())
            max_ts = int(datetime(2030, 1, 1).timestamp())
            
            if timestamp < min_ts or timestamp > max_ts:
                self.logger.error(f"æ—¶é—´æˆ³è¶…å‡ºåˆç†èŒƒå›´: {timestamp}")
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"å‚æ•°éªŒè¯å¤±è´¥: {e}")
            return False

    def get_price_from_history_endpoint(self, coin_id: str, timestamp: int) -> Optional[float]:
        """ä»historyç«¯ç‚¹è·å–ä»·æ ¼ - è¡¥å……ç¼ºå¤±æ–¹æ³•"""
        try:
            target_date = datetime.fromtimestamp(timestamp)
            date_str = target_date.strftime('%d-%m-%Y')
            
            params = {'date': date_str, 'localization': 'false'}
            url = f"{self.base_url}/coins/{coin_id}/history"
            
            self.logger.debug(f"Historyç«¯ç‚¹è¯·æ±‚: {url} - {params}")
            
            response = self.session.get(url, params=params, headers=self.headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                if 'market_data' in data and 'current_price' in data['market_data']:
                    price = data['market_data']['current_price'].get('usd')
                    if price:
                        self.logger.info(f"Historyç«¯ç‚¹æˆåŠŸè·å–ä»·æ ¼: ${price:.4f}")
                        return float(price)
            elif response.status_code == 422:
                self.logger.warning(f"Historyç«¯ç‚¹422é”™è¯¯: {response.text}")
            else:
                self.logger.warning(f"Historyç«¯ç‚¹è¿”å›çŠ¶æ€ç : {response.status_code}")
            
            return None
            
        except Exception as e:
            self.logger.error(f"ä»historyç«¯ç‚¹è·å–ä»·æ ¼å¤±è´¥: {e}")
            return None

    def get_price_from_range_endpoint_fixed(self, coin_id: str, timestamp: int) -> Optional[float]:
        """ä»rangeç«¯ç‚¹è·å–ä»·æ ¼ - ä¿®å¤ç‰ˆï¼Œç¡®ä¿åŒ…å«å¿…éœ€å‚æ•°"""
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆå‰å6å°æ—¶ï¼‰
            start_timestamp = timestamp - 21600  # -6å°æ—¶
            end_timestamp = timestamp + 21600    # +6å°æ—¶
            
            # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å‚æ•° - è¿™æ˜¯ä¿®å¤422é”™è¯¯çš„å…³é”®
            params = {
                'vs_currency': 'usd',
                'from': start_timestamp,
                'to': end_timestamp
            }
            
            url = f"{self.base_url}/coins/{coin_id}/market_chart/range"
            
            self.logger.debug(f"Rangeç«¯ç‚¹è¯·æ±‚: {url}")
            self.logger.debug(f"å‚æ•°: {params}")
            
            response = self.session.get(url, params=params, headers=self.headers, timeout=30)
            
            if response.status_code == 422:
                self.logger.error(f"422é”™è¯¯ - APIå‚æ•°æ— æ•ˆ: {response.text}")
                self.logger.error(f"è¯·æ±‚URL: {url}")
                self.logger.error(f"å‚æ•°: {params}")
                return None
            
            response.raise_for_status()
            
            data = response.json()
            prices = data.get('prices', [])
            
            if prices:
                # æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡æ—¶é—´æˆ³çš„ä»·æ ¼
                target_ms = timestamp * 1000
                closest_price = min(prices, key=lambda x: abs(x[0] - target_ms))
                price = float(closest_price[1])
                self.logger.info(f"Rangeç«¯ç‚¹æˆåŠŸè·å–ä»·æ ¼: ${price:.4f}")
                return price
            
            return None
            
        except Exception as e:
            self.logger.error(f"ä»rangeç«¯ç‚¹è·å–ä»·æ ¼å¤±è´¥: {e}")
            return None

    def get_current_price_as_fallback(self, coin_id: str) -> Optional[float]:
        """è·å–å½“å‰ä»·æ ¼ä½œä¸ºåå¤‡æ–¹æ¡ˆ - è¡¥å……ç¼ºå¤±æ–¹æ³•"""
        try:
            params = {'ids': coin_id, 'vs_currencies': 'usd'}
            url = f"{self.base_url}/simple/price"
            
            response = self.session.get(url, params=params, headers=self.headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                if coin_id in data:
                    price = float(data[coin_id]['usd'])
                    self.logger.info(f"å½“å‰ä»·æ ¼ä½œä¸ºåå¤‡: ${price:.4f}")
                    return price
            
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–å½“å‰ä»·æ ¼å¤±è´¥: {e}")
            return None

    def verify_prediction_with_real_prices(self, prediction: Dict) -> Dict:
        """ä½¿ç”¨çœŸå®å†å²ä»·æ ¼éªŒè¯é¢„æµ‹ - ä¿®å¤ç‰ˆ"""
        try:
            # å®‰å…¨åœ°æå–é¢„æµ‹ä¿¡æ¯
            tweet_info = prediction.get('tweet_info')
            if not tweet_info and 'original_tweet_info' in prediction:
                 tweet_info = prediction['original_tweet_info']
            
            if not tweet_info:
                return {"error": "ç¼ºå°‘æ¨æ–‡ä¿¡æ¯"}

            # å®‰å…¨åœ°è·å–å¿…è¦å­—æ®µ
            coin_id = tweet_info.get('coingecko_id')
            tweet_time = tweet_info.get('tweet_created_at')
            sentiment = prediction.get('sentiment')
            timeframe = prediction.get('timeframe')
            check_points = prediction.get('intelligent_check_points', [])

            # æ•°æ®éªŒè¯
            if not coin_id:
                return {"error": "ç¼ºå°‘coingecko_id"}
            if not tweet_time:
                return {"error": "ç¼ºå°‘tweet_created_at"}
            if not sentiment:
                return {"error": "ç¼ºå°‘sentiment"}

            # ç¡®ä¿check_pointsä¸ä¸ºç©º
            if not check_points:
                check_points = ["24h"]  # é»˜è®¤æ£€æŸ¥ç‚¹

            # è½¬æ¢æ¨æ–‡æ—¶é—´ä¸ºæ—¶é—´æˆ³
            try:
                tweet_timestamp = int(pd.to_datetime(tweet_time).timestamp())
            except Exception as e:
                return {"error": f"æ—¶é—´æˆ³è½¬æ¢å¤±è´¥: {e}"}

            # éªŒè¯å‚æ•° - ä½¿ç”¨ä¿®å¤çš„æ–¹æ³•
            if not self.validate_api_parameters(coin_id, tweet_timestamp):
                return {"error": "å‚æ•°éªŒè¯å¤±è´¥"}

            self.logger.info(f"ğŸ” éªŒè¯é¢„æµ‹: {coin_id} {sentiment} ({timeframe})")
            self.logger.info(f"ğŸ“… æ¨æ–‡æ—¶é—´: {tweet_time} (æ—¶é—´æˆ³: {tweet_timestamp})")
            self.logger.info(f"â° æ™ºèƒ½é€‰æ‹©çš„æ£€æŸ¥ç‚¹: {check_points}")

            # Step 1: è·å–æ¨æ–‡å‘å¸ƒæ—¶çš„åŸºå‡†ä»·æ ¼
            base_price = self.get_precise_historical_price(coin_id, tweet_timestamp)
            if base_price is None:
                return {"error": "æ— æ³•è·å–åŸºå‡†ä»·æ ¼"}

            self.logger.info(f"ğŸ’° æ¨æ–‡æ—¶ä»·æ ¼: ${base_price:.4f}")

            # Step 2: è·å–å„æ£€æŸ¥ç‚¹çš„çœŸå®å†å²ä»·æ ¼
            verification_results = {
                'base_price': base_price,
                'base_timestamp': tweet_timestamp,
                'base_date': tweet_time,
                'coin_id': coin_id,
                'check_points': [],
                'prediction_sentiment': sentiment,
                'prediction_timeframe': timeframe,
                'intelligent_check_points': check_points,
                'specific_claim': prediction.get('specific_claim', '')
            }

            for check_point in check_points:
                # è®¡ç®—ç›®æ ‡æ—¶é—´æˆ³
                target_timestamp = self.calculate_target_timestamp(tweet_timestamp, check_point)
                target_date = datetime.fromtimestamp(target_timestamp)

                self.logger.info(f"â° è·å– {check_point} åä»·æ ¼: {target_date.strftime('%Y-%m-%d %H:%M:%S')}")

                # æ£€æŸ¥ç›®æ ‡æ—¶é—´æ˜¯å¦åœ¨æœªæ¥
                current_timestamp = int(time.time())
                is_future = target_timestamp > current_timestamp

                if is_future:
                    # ç›®æ ‡æ—¶é—´åœ¨æœªæ¥ï¼Œæ ‡è®°ä¸ºå¾…é¢„æµ‹ï¼ˆä¸è®¡å…¥æˆåŠŸ/å¤±è´¥ï¼‰
                    self.logger.info(f"  â³ ç›®æ ‡æ—¶é—´åœ¨æœªæ¥ï¼Œæ ‡è®°ä¸ºå¾…é¢„æµ‹")
                    verification_results['check_points'].append({
                        'check_point': str(check_point),
                        'target_timestamp': target_timestamp,
                        'target_date': target_date.strftime('%Y-%m-%d %H:%M:%S'),
                        'error': 'å¾…é¢„æµ‹',
                        'data_quality': 'pending'
                    })
                else:
                    # ç›®æ ‡æ—¶é—´åœ¨è¿‡å»æˆ–ç°åœ¨ï¼Œè·å–çœŸå®å†å²ä»·æ ¼
                    target_price = self.get_precise_historical_price(coin_id, target_timestamp)

                    if target_price is not None:
                        # è®¡ç®—ä»·æ ¼å˜åŒ–
                        price_change = ((target_price - base_price) / base_price) * 100

                        # åˆ¤æ–­é¢„æµ‹æ˜¯å¦æ­£ç¡®
                        is_correct = self.evaluate_prediction_accuracy(sentiment, price_change)

                        check_result = {
                            'check_point': str(check_point),
                            'target_timestamp': target_timestamp,
                            'target_date': target_date.strftime('%Y-%m-%d %H:%M:%S'),
                            'target_price': target_price,
                            'price_change_percent': price_change,
                            'price_change_absolute': target_price - base_price,
                            'is_correct': is_correct,
                            'evaluation': 'CORRECT' if is_correct else 'INCORRECT',
                            'data_quality': 'verified'
                        }

                        verification_results['check_points'].append(check_result)

                        self.logger.info(f"  ğŸ’µ {check_point}åä»·æ ¼: ${target_price:.4f}")
                        self.logger.info(f"  ğŸ“Š ä»·æ ¼å˜åŒ–: {price_change:+.2f}%")
                        self.logger.info(f"  âœ… é¢„æµ‹ç»“æœ: {'æ­£ç¡®' if is_correct else 'é”™è¯¯'}")
                    else:
                        self.logger.warning(f"  âŒ æ— æ³•è·å–{check_point}åä»·æ ¼ï¼ˆå†å²æ•°æ®ä¸å¯ç”¨ï¼‰")
                        verification_results['check_points'].append({
                            'check_point': str(check_point),
                            'target_timestamp': target_timestamp,
                            'target_date': target_date.strftime('%Y-%m-%d %H:%M:%S'),
                            'error': 'å†å²æ•°æ®ä¸å¯ç”¨',
                            'data_quality': 'failed'
                        })

                # APIé™é¢‘
                time.sleep(self.rate_limit)

            # Step 3: è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
            valid_checks = [cp for cp in verification_results['check_points'] if 'is_correct' in cp]
            pending_checks = [cp for cp in verification_results['check_points'] if cp.get('data_quality') == 'pending']
            correct_predictions = sum(1 for cp in valid_checks if cp['is_correct'])
            total_predictions = len(valid_checks)

            # äºŒå€¼å‡†ç¡®åº¦ï¼šåªè¦ä»»æ„ä¸€ä¸ªæ£€æŸ¥ç‚¹å‘½ä¸­ï¼Œåˆ™è§†ä¸ºé€šè¿‡ï¼ˆ100%ï¼‰ï¼Œå¦åˆ™è§†ä¸ºæœªé€šè¿‡ï¼ˆ0%ï¼‰ã€‚
            # é‡è¦ï¼šå¦‚æœæ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½æ˜¯å¾…é¢„æµ‹çŠ¶æ€ï¼ˆtotal_predictions = 0ï¼‰ï¼Œåˆ™å‡†ç¡®ç‡ä¸º0%
            if total_predictions > 0 and correct_predictions > 0:
                overall_accuracy = 100
                binary_correct_count = 1
            else:
                overall_accuracy = 0
                binary_correct_count = 0

            verification_results['overall_accuracy'] = overall_accuracy
            # ä¸ºå…¼å®¹å†å²å­—æ®µï¼Œä¿ç•™ correct_count/total_countï¼Œä½† correct_count å¯¹åº”äºæ˜¯å¦æœ‰é€šè¿‡ï¼ˆ0/1ï¼‰
            verification_results['correct_count'] = binary_correct_count
            verification_results['total_count'] = total_predictions
            verification_results['pending_count'] = len(pending_checks)
            verification_results['verification_method'] = 'real_timestamp_api'
            verification_results['verification_timestamp'] = datetime.now().isoformat()

            # è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯
            if total_predictions == 0:
                self.logger.info(f"ğŸ¯ éªŒè¯ç»“æœ: å¾…é¢„æµ‹ (æ‰€æœ‰æ£€æŸ¥ç‚¹éƒ½æ˜¯æœªæ¥æ—¶é—´ï¼Œå‡†ç¡®ç‡: {overall_accuracy}%)")
            elif overall_accuracy == 100:
                self.logger.info(f"ğŸ¯ éªŒè¯ç»“æœ: é€šè¿‡ (æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy}%) ({binary_correct_count}/{total_predictions} æ¡æ£€æŸ¥é€šè¿‡)")
            else:
                self.logger.info(f"ğŸ¯ éªŒè¯ç»“æœ: æœªé€šè¿‡ (æ•´ä½“å‡†ç¡®ç‡: {overall_accuracy}%) ({binary_correct_count}/{total_predictions} æ¡æ£€æŸ¥é€šè¿‡)")

            return verification_results

        except Exception as e:
            self.logger.error(f"éªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}

    def get_precise_historical_price(self, coin_id: str, timestamp: int) -> Optional[float]:
        """è·å–ç²¾ç¡®æ—¶é—´æˆ³çš„å†å²ä»·æ ¼ - ä¿®å¤ç‰ˆ"""
        try:
            # å‚æ•°éªŒè¯
            if not self.validate_api_parameters(coin_id, timestamp):
                return None

            # é¦–å…ˆå°è¯•historyç«¯ç‚¹
            price = self.get_price_from_history_endpoint(coin_id, timestamp)
            if price is not None:
                return price

            # ç„¶åå°è¯•rangeç«¯ç‚¹ - ä½¿ç”¨ä¿®å¤ç‰ˆæ–¹æ³•
            price = self.get_price_from_range_endpoint_fixed(coin_id, timestamp)
            if price is not None:
                return price

            # å¦‚æœæ˜¯æœ€è¿‘çš„æ—¶é—´ï¼Œä½¿ç”¨å½“å‰ä»·æ ¼
            if abs(timestamp - int(time.time())) < 7200:  # 2å°æ—¶å†…
                return self.get_current_price_as_fallback(coin_id)
        
            return None
        
        except Exception as e:
            self.logger.error(f"è·å–å†å²ä»·æ ¼å¤±è´¥: {e}")
            return None

    def calculate_target_timestamp(self, base_timestamp: int, check_point: str) -> int:
        """è®¡ç®—æ£€æŸ¥ç‚¹çš„ç›®æ ‡æ—¶é—´æˆ³ - ä¿®å¤ç‰ˆ"""
        if isinstance(check_point, (int, float)):
            return int(check_point)
        
        check_point = str(check_point).strip().lower()
        
        try:
            # æå–æ•°å­—å’Œå•ä½
            match = re.match(r'^(\d+)([a-z]+)$', check_point)
            if not match:
                self.logger.warning(f"æ— æ³•è§£ææ—¶é—´ç‚¹æ ¼å¼: {check_point}, ä½¿ç”¨é»˜è®¤24å°æ—¶")
                return base_timestamp + 86400
            
            num = int(match.group(1))
            unit = match.group(2)
            
            if unit in ['h', 'hr', 'hour', 'hours']:
                candidate = base_timestamp + (num * 3600)
            elif unit in ['d', 'day', 'days']:
                candidate = base_timestamp + (num * 86400)
            elif unit in ['w', 'wk', 'week', 'weeks']:
                candidate = base_timestamp + (num * 604800)
            elif unit in ['m', 'min', 'minute', 'minutes']:
                candidate = base_timestamp + (num * 60)
            else:
                self.logger.warning(f"æœªçŸ¥æ—¶é—´å•ä½: {unit}, é»˜è®¤æŒ‰å°æ—¶å¤„ç†")
                candidate = base_timestamp + (num * 3600)

            # å¼ºåˆ¶ä¸Šé™ï¼šæœ€é•¿ä¸è¶…è¿‡6ä¸ªæœˆï¼ˆæŒ‰183å¤©è¿‘ä¼¼6ä¸ªæœˆï¼‰
            MAX_DELTA_DAYS = 183
            max_allowed = base_timestamp + (MAX_DELTA_DAYS * 86400)
            if candidate > max_allowed:
                self.logger.info(f"æ£€æŸ¥ç‚¹ {check_point} è¶…è¿‡ä¸Šé™ï¼ˆ6ä¸ªæœˆï¼‰ï¼Œå·²æˆªæ–­åˆ° {MAX_DELTA_DAYS} å¤©å")
                return int(max_allowed)

            return int(candidate)
                
        except Exception as e:
            self.logger.warning(f"æ—¶é—´ç‚¹è§£æå¤±è´¥: {check_point}, ä½¿ç”¨é»˜è®¤24å°æ—¶ã€‚é”™è¯¯: {e}")
            return base_timestamp + 86400

    def evaluate_prediction_accuracy(self, sentiment: str, price_change: float) -> bool:
        """è¯„ä¼°é¢„æµ‹å‡†ç¡®æ€§ - ä¿®å¤ç‰ˆ"""
        if sentiment == 'bullish':
            return price_change > 0
        elif sentiment == 'bearish':
            return price_change < 0
        elif sentiment == 'neutral':
            return abs(price_change) < 2
        else:
            return False

# ========================================================================
# CompleteTechnicalAgentEnhanced é›†æˆ - ä¿®å¤ç‰ˆ
# ========================================================================

class CompleteTechnicalAgentEnhanced:
    """å¢å¼ºç‰ˆæŠ€æœ¯æŒ‡æ ‡Agent - çœŸæ­£çš„CoinGecko APIé›†æˆ - ä¿®å¤ç‰ˆ"""
    
    def __init__(self, openai_api_key: str, coingecko_api_key: Optional[str] = None):
        self.openai_client = OpenAI(api_key=openai_api_key)
        self.coingecko_api_key = coingecko_api_key
        
        if coingecko_api_key:
            self.base_url = "https://pro-api.coingecko.com/api/v3"
            self.headers = {"x-cg-pro-api-key": coingecko_api_key}
            self.rate_limit_delay = 0.12
        else:
            self.base_url = "https://api.coingecko.com/api/v3"
            self.headers = {}
            self.rate_limit_delay = 2.5
        
        self.session = requests.Session()
        retry_strategy = Retry(
            total=5,
            backoff_factor=2,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        self.cache = {}
        self.cache_ttl = 300
        
        self.logger = logging.getLogger("CompleteTechnicalAgentEnhanced")
    
    def process_coingecko_query(self, query: str) -> Dict:
        """å¤„ç†CoinGeckoæŸ¥è¯¢ - ä¿®å¤ç‰ˆï¼Œæ”¹è¿›ç»“æœæ ¼å¼åŒ–"""
        try:
            query_lower = query.lower()
            coin_id = self._extract_coin_id_from_query(query)
            
            if not coin_id:
                return {
                    "success": False, 
                    "error": "æ— æ³•ä»æŸ¥è¯¢ä¸­æå–å¸ç§ID", 
                    "query": query, 
                    "search_type": "coingecko_api"
                }
            
            results = []
            detailed_results = {}
            
            # è·å–åŸºç¡€ä»·æ ¼æ•°æ®
            if any(keyword in query_lower for keyword in ['ä»·æ ¼', 'price', 'å†å²', 'history']):
                price_data = self._get_coin_price_data(coin_id)
                if price_data:
                    results.append(f"å½“å‰ä»·æ ¼: ${price_data.get('current_price', 'N/A')}")
                    results.append(f"24hå˜åŒ–: {price_data.get('price_change_percentage_24h', 0):.2f}%")
                    results.append(f"å¸‚å€¼: ${price_data.get('market_cap', 0):,}")
                    detailed_results['price_data'] = price_data
            
            # è·å–æŠ€æœ¯æŒ‡æ ‡æ•°æ®
            if any(keyword in query_lower for keyword in ['æŠ€æœ¯æŒ‡æ ‡', 'technical', 'rsi', 'macd']):
                technical_data = self._get_technical_indicators(coin_id)
                if technical_data:
                    results.extend(technical_data)
                    detailed_results['technical_indicators'] = technical_data
            
            # è·å–å¸‚åœºæ•°æ®
            if any(keyword in query_lower for keyword in ['å¸‚åœº', 'market', 'æ–°é—»', 'news']):
                market_data = self._get_market_data(coin_id)
                if market_data:
                    results.extend(market_data)
                    detailed_results['market_data'] = market_data
            
            # å¦‚æœæ²¡æœ‰ç‰¹å®šè¯·æ±‚ï¼Œè·å–é€šç”¨ä¿¡æ¯
            if not results:
                general_data = self._get_general_coin_info(coin_id)
                if general_data:
                    results = general_data
                    detailed_results['general_info'] = general_data
            
            # ä¿®å¤ï¼šæ ¼å¼åŒ–ç»“æœä»¥é¿å…æˆªæ–­ - è¿™æ˜¯é—®é¢˜5çš„ä¿®å¤
            formatted_results = self._format_detailed_results(query, coin_id, results, detailed_results)
            
            return {
                "success": True, 
                "query": query, 
                "coin_id": coin_id, 
                "results": formatted_results,  # ä½¿ç”¨æ ¼å¼åŒ–åçš„ç»“æœ
                "detailed_results": detailed_results,
                "summary": f"CoinGecko APIæŸ¥è¯¢æˆåŠŸï¼Œè¿”å›{len(results)}é¡¹æ•°æ®", 
                "search_type": "coingecko_api",
                "full_content": "\n".join(formatted_results)  # ç¡®ä¿æœ‰å®Œæ•´å†…å®¹
            }
            
        except Exception as e:
            self.logger.error(f"CoinGeckoæŸ¥è¯¢å¤±è´¥: {query} - {e}")
            return {
                "success": False, 
                "error": str(e), 
                "query": query, 
                "search_type": "coingecko_api", 
                "detailed_error": str(e)
            }
    
    def _format_detailed_results(self, query: str, coin_id: str, results: List[str], detailed_results: Dict) -> List[str]:
        """æ ¼å¼åŒ–è¯¦ç»†ç»“æœï¼Œé¿å…æˆªæ–­ - ä¿®å¤é—®é¢˜5"""
        formatted_results = []
        
        # æ·»åŠ æŸ¥è¯¢ä¿¡æ¯å¤´éƒ¨
        formatted_results.append(f"ğŸ” æŸ¥è¯¢ç›®æ ‡: {query}")
        formatted_results.append(f"ğŸ¯ æœç´¢ç›®çš„: è·å–{coin_id}çš„ç²¾ç¡®æŠ€æœ¯æŒ‡æ ‡å’Œå†å²ä»·æ ¼æ•°æ®ç”¨äºé¢„æµ‹éªŒè¯")
        formatted_results.append("")
        formatted_results.append("ğŸ“Š æ ¸å¿ƒå‘ç°:")
        
        # æ ¼å¼åŒ–æ¯ä¸ªç»“æœ
        for i, result in enumerate(results, 1):
            formatted_results.append(f"  {i}. {result}")
        
        # æ·»åŠ è¯¦ç»†æ•°æ®å±•å¼€
        if 'price_data' in detailed_results:
            price_data = detailed_results['price_data']
            formatted_results.append("")
            formatted_results.append("ğŸ’° è¯¦ç»†ä»·æ ¼æ•°æ®:")
            formatted_results.append(f"  â€¢ å½“å‰ä»·æ ¼: ${price_data.get('current_price', 'N/A')}")
            formatted_results.append(f"  â€¢ 24hå˜åŒ–: {price_data.get('price_change_percentage_24h', 0):.2f}%")
            formatted_results.append(f"  â€¢ 24hæœ€é«˜: ${price_data.get('high_24h', 'N/A')}")
            formatted_results.append(f"  â€¢ 24hæœ€ä½: ${price_data.get('low_24h', 'N/A')}")
            formatted_results.append(f"  â€¢ å¸‚å€¼: ${price_data.get('market_cap', 0):,}")
            formatted_results.append(f"  â€¢ 24hæˆäº¤é‡: ${price_data.get('total_volume', 0):,}")
        
        if 'technical_indicators' in detailed_results:
            tech_data = detailed_results['technical_indicators']
            formatted_results.append("")
            formatted_results.append("ğŸ“ˆ æŠ€æœ¯æŒ‡æ ‡åˆ†æ:")
            for indicator in tech_data:
                formatted_results.append(f"  â€¢ {indicator}")
        
        if 'market_data' in detailed_results:
            market_data = detailed_results['market_data']
            formatted_results.append("")
            formatted_results.append("ğŸŒŠ å¸‚åœºåŠ¨æ€:")
            for data_point in market_data:
                formatted_results.append(f"  â€¢ {data_point}")
        
        # æ·»åŠ æŠ•èµ„æ´å¯Ÿ
        formatted_results.append("")
        formatted_results.append("ğŸ’¡ å…³é”®æ´å¯Ÿ:")
        formatted_results.append("  â€¢ è·å¾—äº†å‡†ç¡®çš„å†å²ä»·æ ¼å’ŒæŠ€æœ¯æŒ‡æ ‡æ•°æ®")
        formatted_results.append("  â€¢ æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ç”¨äºé¢„æµ‹éªŒè¯å’ŒæŠ€æœ¯åˆ†æ")
        formatted_results.append("  â€¢ å»ºè®®ç»“åˆåŸºæœ¬é¢åˆ†æè¿›è¡Œç»¼åˆåˆ¤æ–­")
        
        formatted_results.append("")
        formatted_results.append("âš ï¸ é£é™©å› ç´ :")
        formatted_results.append("  â€¢ å†å²æ•°æ®ä¸èƒ½å®Œå…¨é¢„æµ‹æœªæ¥è¡¨ç°")
        formatted_results.append("  â€¢ æŠ€æœ¯æŒ‡æ ‡å­˜åœ¨æ»åæ€§ï¼Œéœ€è¦ç»“åˆå…¶ä»–åˆ†æ")
        formatted_results.append("  â€¢ å¸‚åœºæ³¢åŠ¨å¯èƒ½å½±å“æŒ‡æ ‡çš„æœ‰æ•ˆæ€§")
        
        formatted_results.append("")
        formatted_results.append("ğŸš€ æŠ•èµ„å¯ç¤º:")
        formatted_results.append("  â€¢ ä½¿ç”¨å¤šä¸ªæŠ€æœ¯æŒ‡æ ‡è¿›è¡Œç»¼åˆåˆ†æ")
        formatted_results.append("  â€¢ ç»“åˆå®è§‚å¸‚åœºç¯å¢ƒè¿›è¡Œåˆ¤æ–­")
        formatted_results.append("  â€¢ è®¾ç½®åˆç†çš„é£é™©æ§åˆ¶æªæ–½")
        
        return formatted_results
    
    def _extract_coin_id_from_query(self, query: str) -> str:
        """ä»æŸ¥è¯¢ä¸­æå–å¸ç§ID - å¢å¼ºç‰ˆ"""
        query_lower = query.lower()
        
        # æ‰©å±•çš„å¸ç§æ˜ å°„ - ä¿®å¤é—®é¢˜3çš„éƒ¨åˆ†è§£å†³æ–¹æ¡ˆ
        coin_mapping = {
            'bitcoin': 'bitcoin', 'btc': 'bitcoin', 
            'ethereum': 'ethereum', 'eth': 'ethereum',
            'pendle': 'pendle', 'solana': 'solana', 
            'sol': 'solana', 'cardano': 'cardano', 
            'ada': 'cardano', 'aave': 'aave', 
            'aero': 'aerodrome-finance',
            'kaito': 'kaito',  # ä¿®å¤kaitoæ˜ å°„
            'ena': 'ethena',
            'syrup': 'syrup',
            'euler': 'euler'
        }
        
        # å…ˆæ£€æŸ¥å®Œæ•´æ˜ å°„
        for name, coin_id in coin_mapping.items():
            if name in query_lower:
                return coin_id
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æå–å•è¯
        words = query.split()
        for word in words:
            word_clean = word.strip('.,!?()[]{}').lower()
            # è¿‡æ»¤æ‰æ˜æ˜¾æ— æ•ˆçš„è¯ - ä¿®å¤é—®é¢˜3
            if len(word_clean) > 2 and word_clean.isalpha():
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ— æ•ˆçš„æµ‹è¯•ID
                if not word_clean.startswith(('xxx_', 'test_', 'sample_', 'mock_', 'demo_', 'fake_')):
                    return word_clean
        
        return None
    
    def _get_coin_price_data(self, coin_id: str) -> Optional[Dict]:
        """è·å–å¸ç§ä»·æ ¼æ•°æ®"""
        try:
            time.sleep(self.rate_limit_delay)  # APIé™é¢‘
            
            url = f"{self.base_url}/coins/{coin_id}"
            response = self.session.get(url, headers=self.headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                market_data = data.get('market_data', {})
                return {
                    'current_price': market_data.get('current_price', {}).get('usd'),
                    'price_change_percentage_24h': market_data.get('price_change_percentage_24h'),
                    'market_cap': market_data.get('market_cap', {}).get('usd'),
                    'total_volume': market_data.get('total_volume', {}).get('usd'),
                    'high_24h': market_data.get('high_24h', {}).get('usd'),
                    'low_24h': market_data.get('low_24h', {}).get('usd')
                }
            elif response.status_code == 404:
                self.logger.warning(f"å¸ç§æœªæ‰¾åˆ°: {coin_id}")
            else:
                self.logger.warning(f"è·å–ä»·æ ¼æ•°æ®çŠ¶æ€ç : {response.status_code}")
            
            return None
            
        except Exception as e:
            self.logger.error(f"è·å–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _get_technical_indicators(self, coin_id: str) -> List[str]:
        """è·å–æŠ€æœ¯æŒ‡æ ‡æ•°æ® - å¢å¼ºç‰ˆ"""
        try:
            # å°è¯•è·å–å†å²æ•°æ®è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            url = f"{self.base_url}/coins/{coin_id}/market_chart"
            params = {'vs_currency': 'usd', 'days': '30'}
            response = self.session.get(url, params=params, headers=self.headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                prices = data.get('prices', [])
                
                if len(prices) >= 14:  # è¶³å¤Ÿè®¡ç®—RSI
                    # ç®€åŒ–çš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
                    price_series = [p[1] for p in prices[-14:]]
                    
                    # ç®€å•RSIè®¡ç®—
                    gains = []
                    losses = []
                    for i in range(1, len(price_series)):
                        change = price_series[i] - price_series[i-1]
                        if change > 0:
                            gains.append(change)
                            losses.append(0)
                        else:
                            gains.append(0)
                            losses.append(abs(change))
                    
                    avg_gain = sum(gains) / len(gains) if gains else 0
                    avg_loss = sum(losses) / len(losses) if losses else 1
                    
                    rs = avg_gain / avg_loss if avg_loss != 0 else 0
                    rsi = 100 - (100 / (1 + rs))
                    
                    # ä»·æ ¼è¶‹åŠ¿
                    recent_change = ((price_series[-1] - price_series[0]) / price_series[0]) * 100
                    
                    return [
                        f"RSI(14): {rsi:.1f} ({'è¶…ä¹°' if rsi > 70 else 'è¶…å–' if rsi < 30 else 'æ­£å¸¸'})",
                        f"è¿‘æœŸè¶‹åŠ¿: {recent_change:+.2f}%",
                        f"ä»·æ ¼åŠ¨é‡: {'ä¸Šæ¶¨' if recent_change > 0 else 'ä¸‹è·Œ'}",
                        f"æ³¢åŠ¨æ€§: {'é«˜' if abs(recent_change) > 10 else 'ä¸­ç­‰' if abs(recent_change) > 5 else 'ä½'}"
                    ]
            
            # åå¤‡æŒ‡æ ‡
            return [
                "RSI(14): è®¡ç®—ä¸­ (éœ€è¦æ›´å¤šå†å²æ•°æ®)", 
                "MACD: æ•°æ®è·å–ä¸­", 
                "å¸ƒæ—å¸¦: åˆ†æä¸­", 
                "æˆäº¤é‡: ç›‘æ§ä¸­"
            ]
            
        except Exception as e:
            self.logger.error(f"è·å–æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            return ["æŠ€æœ¯æŒ‡æ ‡æ•°æ®è·å–å¤±è´¥"]
    
    def _get_market_data(self, coin_id: str) -> List[str]:
        """è·å–å¸‚åœºæ•°æ®"""
        try:
            url = f"{self.base_url}/coins/{coin_id}/market_chart"
            params = {'vs_currency': 'usd', 'days': '7'}
            response = self.session.get(url, params=params, headers=self.headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                prices = data.get('prices', [])
                volumes = data.get('total_volumes', [])
                
                if len(prices) >= 2:
                    week_change = ((prices[-1][1] - prices[0][1]) / prices[0][1]) * 100
                    
                    # æˆäº¤é‡åˆ†æ
                    avg_volume = sum(v[1] for v in volumes) / len(volumes) if volumes else 0
                    recent_volume = volumes[-1][1] if volumes else 0
                    volume_ratio = (recent_volume / avg_volume) if avg_volume > 0 else 1
                    
                    return [
                        f"7æ—¥æ¶¨è·Œ: {week_change:+.2f}%",
                        f"ä»·æ ¼æ³¢åŠ¨: {'é«˜' if abs(week_change) > 10 else 'ä¸­ç­‰' if abs(week_change) > 5 else 'ä½'}",
                        f"è¶‹åŠ¿: {'ä¸Šæ¶¨' if week_change > 0 else 'ä¸‹è·Œ'}",
                        f"æˆäº¤é‡: {'æ”¾é‡' if volume_ratio > 1.2 else 'ç¼©é‡' if volume_ratio < 0.8 else 'æ­£å¸¸'}",
                        f"å¸‚åœºæ´»è·ƒåº¦: {'é«˜' if volume_ratio > 1.5 else 'ä¸­ç­‰' if volume_ratio > 0.8 else 'ä½'}"
                    ]
            
            return ["å¸‚åœºæ•°æ®æš‚æ—¶æ— æ³•è·å–"]
            
        except Exception as e:
            self.logger.error(f"è·å–å¸‚åœºæ•°æ®å¤±è´¥: {e}")
            return ["å¸‚åœºæ•°æ®è·å–å¤±è´¥"]
    
    def _get_general_coin_info(self, coin_id: str) -> List[str]:
        """è·å–é€šç”¨å¸ç§ä¿¡æ¯"""
        try:
            price_data = self._get_coin_price_data(coin_id)
            if price_data:
                return [
                    f"å½“å‰ä»·æ ¼: ${price_data.get('current_price', 'N/A')}",
                    f"24hå˜åŒ–: {price_data.get('price_change_percentage_24h', 0):.2f}%",
                    f"24hæœ€é«˜: ${price_data.get('high_24h', 'N/A')}",
                    f"24hæœ€ä½: ${price_data.get('low_24h', 'N/A')}",
                    f"å¸‚å€¼: ${price_data.get('market_cap', 0):,.0f}",
                    f"24hæˆäº¤é‡: ${price_data.get('total_volume', 0):,.0f}"
                ]
            return ["æ— æ³•è·å–å¸ç§ä¿¡æ¯"]
        except Exception:
            return ["è·å–ä¿¡æ¯æ—¶å‡ºé”™"]

class CoinGeckoAPIQueryHandler:
    """CoinGecko APIæŸ¥è¯¢å¤„ç†å™¨åŒ…è£…å™¨"""
    def __init__(self, openai_api_key: str, coingecko_api_key: Optional[str] = None, use_external: bool = False):
        if use_external and EXTERNAL_AGENT_AVAILABLE:
            self.agent = ExternalTechnicalAgent(openai_api_key, coingecko_api_key)
            self.agent_type = "external"
        else:
            self.agent = CompleteTechnicalAgentEnhanced(openai_api_key, coingecko_api_key)
            self.agent_type = "internal"
        
        self.logger = logging.getLogger("CoinGeckoAPIQueryHandler")
    
    def process_coingecko_query(self, query: str) -> Dict:
        try:
            self.logger.info(f"ä½¿ç”¨{self.agent_type}ä»£ç†å¤„ç†æŸ¥è¯¢: {query}")
            result = self.agent.process_coingecko_query(query)
            self.logger.info(f"æŸ¥è¯¢å®Œæˆï¼ŒæˆåŠŸ: {result.get('success', False)}")
            return result
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return {
                "success": False, 
                "error": str(e), 
                "query": query, 
                "search_type": "coingecko_api_error"
            }

# ========================================================================
# ä¸»åˆ†æå™¨ç±» - ä¼˜åŒ–ç‰ˆï¼Œé…åˆHTMLç”Ÿæˆå™¨
# ========================================================================

class EnhancedKOLAnalyzerV2:
    def __init__(self, db_dir: str, api_key: str, coingecko_api_key: Optional[str] = None):
        self.db_dir = db_dir
        # æ”¯æŒæŠŠæ‰€æœ‰ db å’Œ json æ”¾åœ¨æœ¬æ¨¡å—ä¸Šä¸€çº§çš„ data/ ç›®å½•
        module_parent = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
        self.parent_data_dir = os.path.join(module_parent, 'data')

        # é¦–é€‰ä½¿ç”¨ä¼ å…¥çš„ db_dir ä¸‹çš„æ•°æ®åº“æ–‡ä»¶ï¼›å¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ°ä¸Šä¸€çº§çš„ data/ ä¸‹
        self.crypto_db_path = os.path.join(db_dir, "crypto_recommendations.db")
        if not os.path.exists(self.crypto_db_path):
            alt_db = os.path.join(self.parent_data_dir, "crypto_recommendations.db")
            if os.path.exists(alt_db):
                self.crypto_db_path = alt_db

        self.output_dir = os.path.join(db_dir, "enhanced_kol_reports_v2_optimized")
        self.api_key = api_key
        self.coingecko_api_key = coingecko_api_key
        
        self.ensure_output_directories()
        self._setup_logging()
        self.client = OpenAI(api_key=api_key)
        
        self._init_technical_agent()
        self.verification_engine = RealTimestampVerificationEngine(coingecko_api_key)
        # å°† CoinGecko API key ä¼ å…¥ HTML ç”Ÿæˆå™¨ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªå¸ç§é¡µé¢é™„åŠ  CoinGecko é“¾æ¥
        self.html_generator = HTMLReportGenerator(template_dir=db_dir, coingecko_api_key=coingecko_api_key)
        
        self.last_request_time = 0
        self.min_request_interval = 60 / MAX_REQUESTS_PER_MINUTE
        
        # å¹¶å‘ä¸å¼‚æ­¥OpenAIå®¢æˆ·ç«¯ï¼ˆç”¨äºå¹¶è¡Œæ¨æ–‡åˆ†æï¼‰
        self.max_concurrent_tweet_analysis = 3  # å¯æ ¹æ®é…é¢è°ƒæ•´
        self.per_request_jitter_secs = (0.1, 0.4)
        self.async_client = AsyncOpenAI(api_key=api_key)
        
        # åŠ è½½KOL profiles
        self._load_kol_profiles()

    def ensure_output_directories(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        directories = [
            self.output_dir,
            os.path.join(self.output_dir, "kol_analysis"),
            os.path.join(self.output_dir, "charts")
        ]
        
        for directory in directories:
            try:
                os.makedirs(directory, exist_ok=True)
            except Exception as e:
                print(f"âŒ ç›®å½•åˆ›å»ºæˆ–æƒé™æ£€æŸ¥å¤±è´¥: {directory}, é”™è¯¯: {e}")
                raise

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        log_file = os.path.join(self.output_dir, f"analyzer_v2_optimized_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[logging.FileHandler(log_file), logging.StreamHandler()]
        )
        self.logger = logging.getLogger("KOLAnalyzerV2Optimized")
        self.logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")

    def _init_technical_agent(self):
        """åˆå§‹åŒ–CompleteTechnicalAgentEnhanced"""
        try:
            self.technical_agent = CoinGeckoAPIQueryHandler(
                self.api_key, 
                self.coingecko_api_key, 
                use_external=EXTERNAL_AGENT_AVAILABLE
            )
            self.logger.info(f"âœ… ä½¿ç”¨{'å¤–éƒ¨' if EXTERNAL_AGENT_AVAILABLE else 'å†…ç½®'}CompleteTechnicalAgentåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"âš ï¸ CompleteTechnicalAgentåˆå§‹åŒ–å¤±è´¥: {e}")
            self.technical_agent = None
    
    def _load_kol_profiles(self):
        """åŠ è½½KOL profileæ•°æ®"""
        try:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„ï¼ˆä¼˜å…ˆdb_dirï¼Œå…¶æ¬¡db_dirçš„çˆ¶ç›®å½•ï¼Œå†æ¬¡æ¨¡å—ä¸Šä¸€çº§çš„ data/ï¼‰
            possible_paths = [
                os.path.join(self.db_dir, 'kol_list.json'),
                os.path.join(os.path.dirname(self.db_dir), 'kol_list.json'),
                os.path.join(self.parent_data_dir, 'kol_list.json'),
                os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data', 'kol_list.json')),
                './kol_list.json'
            ]

            loaded = False
            for profile_path in possible_paths:
                profile_path = os.path.abspath(profile_path)
                if os.path.exists(profile_path):
                    with open(profile_path, 'r', encoding='utf-8') as f:
                        profiles = json.load(f)
                        # ä»¥ username å°å†™ä¸ºé”®ï¼Œæ–¹ä¾¿åç»­æŸ¥æ‰¾
                        self.kol_profiles = {p.get('username', '').lower(): p for p in profiles if isinstance(p, dict) and 'username' in p}
                        self.logger.info(f"âœ… åŠ è½½äº† {len(self.kol_profiles)} ä¸ªKOL profile (from {profile_path})")
                        loaded = True
                        break

            if not loaded:
                self.kol_profiles = {}
                self.logger.warning("âš ï¸ æœªåœ¨å€™é€‰è·¯å¾„ä¸­æ‰¾åˆ° kol_list.jsonï¼›è¯·å°† kol_list.json æ”¾åˆ° db_dir æˆ– ä¸Šä¸€çº§ data/ æ–‡ä»¶å¤¹ä¸­ (e.g. ../data/kol_list.json)ã€‚")

        except Exception as e:
            self.logger.error(f"åŠ è½½KOL profileså¤±è´¥: {e}")
            self.kol_profiles = {}

    def _rate_limit_delay(self):
        """æ§åˆ¶APIè¯·æ±‚é¢‘ç‡"""
        elapsed = time.time() - self.last_request_time
        if elapsed < self.min_request_interval:
            sleep_time = self.min_request_interval - elapsed
            self.logger.info(f"APIé™é¢‘ç­‰å¾… {sleep_time:.1f} ç§’...")
            time.sleep(sleep_time)
        self.last_request_time = time.time()

    async def _async_rate_limit_delay(self, call_type: str = 'standard'):
        """å¼‚æ­¥é™é¢‘å™¨ï¼šæŒ‰è°ƒç”¨ç±»å‹çš„æœ€å°å¿…è¦é—´éš” + è½»å¾®æŠ–åŠ¨ï¼Œä¸é˜»å¡äº‹ä»¶å¾ªç¯"""
        # ç±»å‹åŒ–æœ€å°é—´éš”ï¼ˆç§’ï¼‰ï¼šè¯·æŒ‰å®é™…APIé™é€Ÿè°ƒå‚
        type_min_delays = {
            'standard': 0.6,   # æ™®é€šè°ƒç”¨ï¼ˆå¦‚å•æ¡æ¨æ–‡åˆ†æï¼‰
            'batch': 1.0,      # æ‰¹é‡/èšåˆè°ƒç”¨ï¼ˆå¦‚æ‰¹é‡æœç´¢/éªŒè¯ï¼‰
            'critical': 2.0    # å®¹é”™è¦æ±‚é«˜çš„å…³é”®è°ƒç”¨
        }

        # å…¨å±€ç¡¬é˜ˆå€¼ï¼ˆä¸ä½äºå¹³å°é™åˆ¶ï¼‰ï¼Œä¸ç±»å‹åŒ–é—´éš”å–è¾ƒå¤§è€…
        global_min = getattr(self, 'min_request_interval', 0.0)
        type_min = type_min_delays.get(call_type, type_min_delays['standard'])
        required_delay = max(global_min, type_min)

        elapsed = time.time() - self.last_request_time
        if elapsed < required_delay:
            await asyncio.sleep(required_delay - elapsed)

        # è½»å¾®æŠ–åŠ¨ï¼Œå‡æ‘Šçªåˆºï¼Œé™ä½429æ¦‚ç‡
        jitter_low, jitter_high = getattr(self, 'per_request_jitter_secs', (0.1, 0.4))
        await asyncio.sleep(random.uniform(jitter_low, jitter_high))

        self.last_request_time = time.time()

    def load_reasoning_chains(self) -> Dict[str, List[Dict]]:
        """æŒ‰æ¨ç†é“¾(KOL, å¸ç§)åŠ è½½é¢„æµ‹æ•°æ®"""
        try:
            if not os.path.exists(self.crypto_db_path):
                self.logger.error(f"æ¨èæ•°æ®åº“ {self.crypto_db_path} ä¸å­˜åœ¨")
                return {}

            with sqlite3.connect(self.crypto_db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                cursor.execute('''
                    SELECT id, tweet_id, author_id, author_name, tweet_created_at, 
                           crypto_name, coingecko_id, full_tweet_text, investment_horizon
                    FROM crypto_recommendations
                    WHERE coingecko_id IS NOT NULL 
                      AND coingecko_id NOT IN ('NOT_FOUND', 'ERROR', '', 'N/A')
                      AND LENGTH(coingecko_id) > 0
                      AND tweet_created_at IS NOT NULL
                    ORDER BY author_name, crypto_name, tweet_created_at ASC
                ''')
                recommendations = [dict(row) for row in cursor.fetchall()]

            reasoning_chains = {}
            for rec in recommendations:
                if rec['crypto_name'].upper() in EXCLUDED_ASSETS:
                    continue
                
                # ä½¿ç”¨ä¸“ç”¨åˆ†éš”ç¬¦é¿å…ç”¨æˆ·åæˆ–å¸ç§åä¸­åŒ…å«ä¸‹åˆ’çº¿å¯¼è‡´çš„è§£æé”™è¯¯
                chain_key = f"{rec['author_name']}|||{rec['crypto_name']}"
                reasoning_chains.setdefault(chain_key, []).append(rec)

            # ä¿®æ”¹ï¼šæ¥å—ä»»æ„æœ‰è®°å½•çš„ author+coin ç»„åˆï¼ˆåŒ…æ‹¬åªæœ‰1æ¡è®°å½•çš„æƒ…å†µï¼‰ï¼Œ
            # ä½†ä¼˜å…ˆæŒ‰ Fully Diluted Valuation (FDV) è¿›è¡Œåˆç­›ï¼Œæ’é™¤ FDV æ˜ç¡®ä½äº $1,000,000 çš„å¸ç§ï¼Œ
            # å‡å°‘åç»­ä¸å¿…è¦çš„è®¡ç®—å¼€é”€ã€‚
            MIN_FDV_USD = 1_000_000
            filtered_chains = {}

            # ä½¿ç”¨ verification_engine çš„ session å’Œ base_url å»æŸ¥è¯¢ CoinGeckoï¼ˆè‹¥å¯ç”¨ï¼‰
            session = None
            base_url = None
            headers = {}
            try:
                if hasattr(self, 'verification_engine') and self.verification_engine is not None:
                    session = getattr(self.verification_engine, 'session', None)
                    base_url = getattr(self.verification_engine, 'base_url', None)
                    headers = getattr(self.verification_engine, 'headers', {}) or {}
            except Exception:
                session = None
                base_url = None

            # ä¸ºæ¯ä¸ªé“¾å°è¯•è·å–FDVï¼ˆè‹¥æ— æ³•è·å–åˆ™ä¿å®ˆæ”¾è¡Œï¼‰
            for chain_key, recs in reasoning_chains.items():
                coingecko_id = recs[0].get('coingecko_id')
                exclude_due_to_fdv = False

                if coingecko_id and session and base_url:
                    try:
                        url = f"{base_url}/coins/{coingecko_id}"
                        params = {
                            'localization': 'false',
                            'tickers': 'false',
                            'community_data': 'false',
                            'developer_data': 'false',
                            'sparkline': 'false'
                        }
                        resp = session.get(url, params=params, headers=headers, timeout=15)
                        if resp.status_code == 200:
                            data = resp.json()
                            market_data = data.get('market_data', {}) or {}
                            fdv_map = market_data.get('fully_diluted_valuation', {}) or {}
                            fdv_usd = fdv_map.get('usd')
                            if isinstance(fdv_usd, (int, float)):
                                if fdv_usd < MIN_FDV_USD:
                                    exclude_due_to_fdv = True
                                    self.logger.info(f"æ’é™¤ {coingecko_id}ï¼ˆFDV=${fdv_usd:,}ï¼‰: å°äº ${MIN_FDV_USD:,}")
                        else:
                            # é200å“åº”ï¼Œè®°å½•ä½†ä¸æ’é™¤
                            self.logger.debug(f"æŸ¥è¯¢CoinGecko {coingecko_id} FDVæ—¶è¿”å›çŠ¶æ€ {resp.status_code}")
                    except Exception as e:
                        self.logger.debug(f"æŸ¥è¯¢CoinGecko FDVå¤±è´¥ ({coingecko_id}): {e}")

                if not exclude_due_to_fdv:
                    filtered_chains[chain_key] = recs

            self.logger.info(f"æ‰¾åˆ° {len(filtered_chains)} æ¡æ¨ç†é“¾ï¼ˆFDVç­›é€‰åï¼‰ï¼Œæ€»è®¡ {sum(len(v) for v in filtered_chains.values())} æ¡é¢„æµ‹")
            return filtered_chains
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨ç†é“¾æ•°æ®æ—¶å‡ºé”™: {e}")
            return {}

    # ========================================================================
    # åˆ†ææ–¹æ³• - ä¼˜åŒ–ç‰ˆ
    # ========================================================================
    
    def preprocess_reasoning_chain(self, reasoning_chain: List[Dict], kol_name: str, coin_name: str) -> Dict:
        """ä½¿ç”¨gpt-4o-miniè¯¦ç»†åˆ†ææ•´ä¸ªé€»è¾‘é“¾æ¡èƒŒæ™¯"""
        try:
            self._rate_limit_delay()
            
            # æ„å»ºé“¾æ¡æ‘˜è¦
            chain_summary = []
            for i, tweet in enumerate(reasoning_chain):
                tweet_text = tweet['full_tweet_text']
                if len(tweet_text) > 200:
                    tweet_text = tweet_text[:200] + "..."
                
                chain_summary.append(f"æ¨æ–‡{i+1} ({tweet['tweet_created_at'][:10]}): {tweet_text}")
            
            chain_text = "\n\n".join(chain_summary)
            
            # ä½¿ç”¨promptæ¨¡æ¿
            preprocess_prompt = get_prompt_template(
                'preprocess_chain',
                kol_name=kol_name,
                coin_name=coin_name,
                start_date=reasoning_chain[0]['tweet_created_at'][:10],
                end_date=reasoning_chain[-1]['tweet_created_at'][:10],
                chain_text=chain_text,
                start_date_short=reasoning_chain[0]['tweet_created_at'][:10],
                coin_name_dup=coin_name,
                coin_name_dup2=coin_name
            )

            response = self.client.chat.completions.create(
                model=MINI_MODEL,
                messages=[{"role": "user", "content": preprocess_prompt}],
                response_format={"type": "json_object"},
            )
            
            result_text = response.choices[0].message.content
            chain_context = json.loads(result_text)
            chain_context['raw_response'] = result_text
            
            self.logger.info(f"âœ… æ¨ç†é“¾æ¡è¯¦ç»†é¢„å¤„ç†å®Œæˆ: {chain_context.get('kol_overall_stance', 'unknown')}")
            return chain_context
            
        except Exception as e:
            self.logger.error(f"æ¨ç†é“¾æ¡é¢„å¤„ç†å‡ºé”™: {e}")
            return {
                "kol_overall_stance": "unknown",
                "key_themes": [],
                "sentiment_evolution": "unknown", 
                "context_summary": "é¢„å¤„ç†å¤±è´¥",
                "prediction_pattern": "unknown",
                "typical_timeframes": ["short_term"],
                "analysis_style": "unknown"
            }

    def super_analyzer_with_professional_depth(self, tweet: Dict, chain_context: Dict, coin_name: str) -> Dict:
        """ä¸“ä¸šçº§Super Analyzer - æ·±åº¦åˆ†ææ¯æ¡æ¨æ–‡ - å¢å¼ºç‰ˆ"""
        try:
            self._rate_limit_delay()
            
            tweet_text = tweet['full_tweet_text']
            tweet_time = tweet['tweet_created_at']
            coingecko_id = tweet['coingecko_id']
            
            # è½¬æ¢æ—¶é—´æˆ³
            tweet_timestamp = int(pd.to_datetime(tweet_time).timestamp())
            
            # ä½¿ç”¨å¢å¼ºçš„promptæ¨¡æ¿ - ç¡®ä¿åŒ…å«æ·±åº¦è¯„ä¼°
            super_prompt = get_prompt_template(
                'super_analyzer',
                coin_name=coin_name,
                tweet_datetime=datetime.fromtimestamp(tweet_timestamp).strftime('%Y-%m-%d %H:%M:%S'),
                tweet_time=tweet_time,
                tweet_timestamp=tweet_timestamp,
                chain_context=json.dumps(chain_context, ensure_ascii=False, indent=2),
                tweet_text=tweet_text,
                coin_name_context=coin_name,
                coin_name_query=coin_name,
                tweet_time_query=tweet_time,
                tweet_timestamp_query=tweet_timestamp,
                coin_name_search=coin_name,
                tweet_time_search=tweet_time,
                tweet_id=tweet['tweet_id'],
                author_name=tweet['author_name'],
                coin_name_info=coin_name,
                coingecko_id_info=coingecko_id,
                tweet_time_info=tweet_time,
                tweet_text_info=tweet_text,
                coin_name_sector=coin_name,
                typical_timeframes=chain_context.get('typical_timeframes', {}).get('preferred_horizons', ['çŸ­æœŸ']),
                analysis_style=chain_context.get('analysis_style', {}).get('primary_method', 'mixed'),
                coin_name_features=coin_name
            )

            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": super_prompt}],
                response_format={"type": "json_object"},
            )
            
            result_text = response.choices[0].message.content
            analysis_result = json.loads(result_text)
            analysis_result['raw_response'] = result_text
            analysis_result['tweet_info'] = tweet
            
            self.logger.info(f"âœ… ä¸“ä¸šçº§Super Analyzerå®Œæˆ: {analysis_result.get('content_type', 'unknown')} - {len(analysis_result.get('predictions', []))}")
            
            # è®°å½•è¯¦ç»†åˆ†æç»“æœ
            for pred in analysis_result.get('predictions', []):
                check_points = pred.get('intelligent_check_points', [])
                reasoning = pred.get('time_selection_reasoning', '')
                logic = pred.get('prediction_logic', {})
                self.logger.info(f"  ğŸ¯ æ™ºèƒ½é€‰æ‹©æ—¶é—´ç‚¹: {check_points}")
                self.logger.info(f"  ğŸ’­ é€‰æ‹©ç†ç”±: {reasoning[:100] if reasoning else 'N/A'}")
                tech_basis = logic.get('technical_basis', 'N/A')
                fund_basis = logic.get('fundamental_basis', 'N/A')
                sent_basis = logic.get('sentiment_basis', 'N/A')
                self.logger.info(f"  ğŸ§  é¢„æµ‹é€»è¾‘: æŠ€æœ¯é¢-{tech_basis[:50] if tech_basis else 'N/A'}, åŸºæœ¬é¢-{fund_basis[:50] if fund_basis else 'N/A'}, æƒ…ç»ªé¢-{sent_basis[:50] if sent_basis else 'N/A'}")
            
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"ä¸“ä¸šçº§Super Analyzerå‡ºé”™: {e}")
            return {
                "content_type": "error",
                "predictions": [],
                "analysis_reasoning": f"ä¸“ä¸šçº§åˆ†æå¤±è´¥: {str(e)}",
                "tweet_info": tweet
            }

    async def super_analyzer_with_professional_depth_async(self, tweet: Dict, chain_context: Dict, coin_name: str) -> Dict:
        """ä¸“ä¸šçº§Super Analyzerï¼ˆå¼‚æ­¥ç‰ˆï¼Œç”¨äºå¹¶è¡Œæ¨æ–‡åˆ†æï¼‰"""
        try:
            await self._async_rate_limit_delay()

            tweet_text = tweet['full_tweet_text']
            tweet_time = tweet['tweet_created_at']
            coingecko_id = tweet['coingecko_id']

            tweet_timestamp = int(pd.to_datetime(tweet_time).timestamp())

            super_prompt = get_prompt_template(
                'super_analyzer',
                coin_name=coin_name,
                tweet_datetime=datetime.fromtimestamp(tweet_timestamp).strftime('%Y-%m-%d %H:%M:%S'),
                tweet_time=tweet_time,
                tweet_timestamp=tweet_timestamp,
                chain_context=json.dumps(chain_context, ensure_ascii=False, indent=2),
                tweet_text=tweet_text,
                coin_name_context=coin_name,
                coin_name_query=coin_name,
                tweet_time_query=tweet_time,
                tweet_timestamp_query=tweet_timestamp,
                coin_name_search=coin_name,
                tweet_time_search=tweet_time,
                tweet_id=tweet['tweet_id'],
                author_name=tweet['author_name'],
                coin_name_info=coin_name,
                coingecko_id_info=coingecko_id,
                tweet_time_info=tweet_time,
                tweet_text_info=tweet_text,
                coin_name_sector=coin_name,
                typical_timeframes=chain_context.get('typical_timeframes', {}).get('preferred_horizons', ['çŸ­æœŸ']),
                analysis_style=chain_context.get('analysis_style', {}).get('primary_method', 'mixed'),
                coin_name_features=coin_name
            )

            response = await self.async_client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": super_prompt}],
                response_format={"type": "json_object"},
            )
            result_text = response.choices[0].message.content
            analysis_result = json.loads(result_text)
            analysis_result['raw_response'] = result_text
            analysis_result['tweet_info'] = tweet

            self.logger.info(f"âœ… (å¹¶å‘) Super Analyzerå®Œæˆ: {analysis_result.get('content_type', 'unknown')} - {len(analysis_result.get('predictions', []))}")
            return analysis_result

        except Exception as e:
            self.logger.error(f"(å¹¶å‘) Super Analyzerå‡ºé”™: {e}")
            return {
                "content_type": "error",
                "predictions": [],
                "analysis_reasoning": f"å¹¶å‘åˆ†æå¤±è´¥: {str(e)}",
                "tweet_info": tweet
            }

    async def analyze_tweets_concurrently(self, reasoning_chain: List[Dict], chain_context: Dict, coin_name: str) -> List[Dict]:
        """å¹¶è¡Œåˆ†ææ¨æ–‡ï¼Œæé€ŸPhase 2"""
        semaphore = asyncio.Semaphore(self.max_concurrent_tweet_analysis)
        self.logger.info(f"å¼€å§‹å¹¶è¡Œåˆ†æ {len(reasoning_chain)} æ¡æ¨æ–‡ï¼Œæœ€å¤§å¹¶å‘={self.max_concurrent_tweet_analysis}")

        async def _run_one(tweet: Dict, idx: int):
            async with semaphore:
                try:
                    result = await self.super_analyzer_with_professional_depth_async(tweet, chain_context, coin_name)
                    self.logger.info(f"æ¨æ–‡{idx+1}å¹¶å‘åˆ†æå®Œæˆ: {result.get('content_type', 'unknown')}")
                    return result
                except Exception as e:
                    self.logger.error(f"æ¨æ–‡{idx+1}å¹¶å‘åˆ†æå¤±è´¥: {e}")
                    return {"content_type": "error", "predictions": [], "tweet_info": tweet}

        tasks = [asyncio.create_task(_run_one(tweet, i)) for i, tweet in enumerate(reasoning_chain)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_predictions: List[Dict] = []
        for result in results:
            if isinstance(result, dict) and result.get('content_type') == 'prediction':
                predictions = result.get('predictions', [])
                for pred in predictions:
                    pred['tweet_info'] = result['tweet_info']
                    pred['content_analysis'] = result.get('content_analysis', {})
                    pred['market_context_analysis'] = result.get('market_context_analysis', {})
                    pred['kol_behavioral_analysis'] = result.get('kol_behavioral_analysis', {})
                all_predictions.extend(predictions)

        self.logger.info(f"å¹¶è¡Œåˆ†æå®Œæˆï¼Œå…±æå– {len(all_predictions)} ä¸ªé¢„æµ‹")
        return all_predictions
    async def execute_requests_with_deep_analysis(self, predictions_with_requests: List[Dict]) -> List[Dict]:
        """æ‰§è¡Œæ‰€æœ‰requestså¹¶è¿›è¡Œæ·±åº¦ç»“æœåˆ†æ - ä¿®å¤ç‰ˆ"""
        try:
            self.logger.info("å¼€å§‹æ‰§è¡Œæ‰€æœ‰search requests + æ·±åº¦ç»“æœåˆ†æ...")
            
            prediction_results = []
            
            for prediction in predictions_with_requests:
                self.logger.info(f"å¤„ç†é¢„æµ‹: {prediction['timeframe']} {prediction['sentiment']}")
                
                # Step 1: æ‰§è¡Œsearch_requests
                request_results = []
                search_requests = prediction.get('search_requests', [])
                
                for request in search_requests:
                    try:
                        request_type = request['type']
                        query = request['query']
                        
                        if request_type == 'coingecko_api' and self.technical_agent:
                            result = self.technical_agent.process_coingecko_query(query)
                            request_results.append({
                                'request': request,
                                'result': result,
                                'status': 'success' if result.get('success') else 'failed'
                            })
                            
                        elif request_type == 'web_search':
                            result = await self._execute_enhanced_web_search(query)
                            request_results.append({
                                'request': request, 
                                'result': result,
                                'status': 'success' if result.get('success') else 'failed'
                            })
                            
                        await asyncio.sleep(1)
                        
                    except Exception as e:
                        self.logger.error(f"Requestæ‰§è¡Œå¤±è´¥: {query} - {e}")
                        request_results.append({
                            'request': request,
                            'result': {'success': False, 'error': str(e)},
                            'status': 'failed'
                        })
                
                # Step 2: æ·±åº¦åˆ†æsearch results
                results_analysis = await self._deep_analyze_search_results(request_results, prediction)
                
                # Step 3: æ‰§è¡ŒçœŸå®æ—¶é—´æˆ³éªŒè¯ - ä½¿ç”¨ä¿®å¤çš„éªŒè¯å¼•æ“
                self.logger.info("æ‰§è¡ŒçœŸå®æ—¶é—´æˆ³éªŒè¯...")
                real_verification = self.verification_engine.verify_prediction_with_real_prices(prediction)
                
                # Step 4: ç»¼åˆåˆ†æ - ä½¿ç”¨å¢å¼ºçš„prompt
                comprehensive_analysis = await self._comprehensive_prediction_analysis(
                    prediction, request_results, results_analysis, real_verification
                )
                
                # ç»„è£…é¢„æµ‹ç»“æœ
                prediction_result = prediction.copy()
                prediction_result['request_results'] = request_results
                prediction_result['results_analysis'] = results_analysis
                prediction_result['real_verification'] = real_verification
                prediction_result['comprehensive_analysis'] = comprehensive_analysis
                prediction_result['successful_requests'] = len([r for r in request_results if r['status'] == 'success'])
                prediction_result['total_requests'] = len(request_results)
                prediction_result['verification_status'] = 'completed' if 'error' not in real_verification else 'failed'
                
                prediction_results.append(prediction_result)
                
                self.logger.info(f"é¢„æµ‹{prediction['prediction_id']}æ·±åº¦åˆ†æå®Œæˆ: {prediction_result['successful_requests']}/{prediction_result['total_requests']} requestsæˆåŠŸ, çœŸå®éªŒè¯: {'æˆåŠŸ' if prediction_result['verification_status'] == 'completed' else 'å¤±è´¥'}")
                
                await asyncio.sleep(3)
            
            self.logger.info(f"âœ… æ‰€æœ‰requests + æ·±åº¦åˆ†æå®Œæˆï¼Œå…±{len(prediction_results)}ä¸ªé¢„æµ‹")
            return prediction_results
            
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œrequests + æ·±åº¦åˆ†ææ—¶å‡ºé”™: {e}")
            return []

    async def _execute_enhanced_web_search(self, query: str) -> Dict:
        """æ‰§è¡Œå¢å¼ºç‰ˆwebæœç´¢ - ä¿®å¤ç»“æœå±•ç¤º"""
        try:
            self._rate_limit_delay()
    
            search_prompt = get_prompt_template('web_search', query=query)

            response = self.client.chat.completions.create(
                model=SEARCH_MODEL,
                messages=[{"role": "user", "content": search_prompt}],
            )
            
            search_result = response.choices[0].message.content
            
            # ä¿®å¤ï¼šæ ¼å¼åŒ–æœç´¢ç»“æœï¼Œé¿å…æˆªæ–­ - è§£å†³é—®é¢˜5
            formatted_result = self._format_web_search_result(query, search_result)
            
            return {
                'success': True,
                'query': query,
                'results': [formatted_result],
                'summary': search_result[:300] + "..." if len(search_result) > 300 else search_result,
                'search_type': 'enhanced_web_search',
                'analysis_depth': 'deep',
                'model_used': SEARCH_MODEL,
                'full_content': formatted_result  # å®Œæ•´å†…å®¹ï¼Œä¸æˆªæ–­
            }
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºç‰ˆWebæœç´¢å¤±è´¥: {query} - {e}")
            return {
                'success': False,
                'query': query,
                'error': str(e),
                'search_type': 'enhanced_web_search'
            }
    
    def _format_web_search_result(self, query: str, search_result: str) -> str:
        """æ ¼å¼åŒ–Webæœç´¢ç»“æœï¼Œç¡®ä¿å®Œæ•´å±•ç¤º - ä¿®å¤é—®é¢˜5"""
        formatted_lines = []
        
        # æ·»åŠ æ ‡å‡†åŒ–å¤´éƒ¨
        formatted_lines.append(f"ğŸ” æŸ¥è¯¢ç›®æ ‡: {query}")
        formatted_lines.append("ğŸ¯ æœç´¢ç›®çš„: äº†è§£å¸‚åœºèƒŒæ™¯å’ŒåŸºæœ¬é¢å› ç´ ")
        formatted_lines.append("")
        formatted_lines.append("ğŸ“Š æ ¸å¿ƒå‘ç°:")
        
        # ç¡®ä¿æœç´¢ç»“æœå®Œæ•´å±•ç¤ºï¼Œä¸æˆªæ–­
        if search_result:
            # æŒ‰æ®µè½åˆ†å‰²ç»“æœï¼Œç¡®ä¿å®Œæ•´æ€§
            paragraphs = search_result.split('\n\n')
            for i, paragraph in enumerate(paragraphs, 1):
                if paragraph.strip():
                    formatted_lines.append(f"  {i}. {paragraph.strip()}")
        
        formatted_lines.append("")
        formatted_lines.append("ğŸ’¡ å…³é”®æ´å¯Ÿ:")
        formatted_lines.append("  â€¢ åŸºäºæœ€æ–°å¸‚åœºä¿¡æ¯çš„ä¸“ä¸šåˆ†æ")
        formatted_lines.append("  â€¢ ç»“åˆå¤šä¸ªä¿¡æ¯æºçš„ç»¼åˆåˆ¤æ–­")
        formatted_lines.append("  â€¢ ä¸ºæŠ•èµ„å†³ç­–æä¾›é‡è¦å‚è€ƒ")
        
        formatted_lines.append("")
        formatted_lines.append("âš ï¸ é£é™©å› ç´ :")
        formatted_lines.append("  â€¢ å¸‚åœºä¿¡æ¯å¯èƒ½å­˜åœ¨æ»åæ€§")
        formatted_lines.append("  â€¢ éœ€è¦ç»“åˆæŠ€æœ¯åˆ†æè¿›è¡ŒéªŒè¯")
        formatted_lines.append("  â€¢ å…³æ³¨å®è§‚ç¯å¢ƒå˜åŒ–çš„å½±å“")
        
        formatted_lines.append("")
        formatted_lines.append("ğŸš€ æŠ•èµ„å¯ç¤º:")
        formatted_lines.append("  â€¢ å¯†åˆ‡å…³æ³¨åŸºæœ¬é¢å˜åŒ–")
        formatted_lines.append("  â€¢ ç»“åˆæŠ€æœ¯é¢è¿›è¡Œç»¼åˆåˆ†æ")
        formatted_lines.append("  â€¢ åˆ¶å®šåˆç†çš„é£é™©ç®¡ç†ç­–ç•¥")
        
        return "\n".join(formatted_lines)

    async def _deep_analyze_search_results(self, request_results: List[Dict], prediction: Dict) -> Dict:
        """æ·±åº¦åˆ†æsearch results - ä½¿ç”¨å¢å¼ºçš„prompt"""
        try:
            self._rate_limit_delay()
            
            # æ•´ç†search results
            successful_results = [r for r in request_results if r['status'] == 'success']
            if not successful_results:
                return {"analysis": "æ— æœ‰æ•ˆsearch resultsè¿›è¡Œåˆ†æ", "insights": [], "supporting_evidence": []}
            
            results_summary = []
            for result in successful_results:
                request = result['request']
                result_data = result['result']
                
                # ç¡®ä¿åŒ…å«å®Œæ•´å†…å®¹
                full_content = result_data.get('full_content', '')
                if not full_content:
                    full_content = '\n'.join(result_data.get('results', []))
                
                results_summary.append({
                    "query": request['query'],
                    "purpose": request['purpose'],
                    "type": request['type'],
                    "results": result_data.get('results', []),
                    "summary": result_data.get('summary', 'N/A'),
                    "full_content": full_content  # åŒ…å«å®Œæ•´å†…å®¹
                })
            
            # ä½¿ç”¨å¢å¼ºçš„promptæ¨¡æ¿
            analysis_prompt = get_prompt_template(
                'search_analysis',
                specific_claim=prediction.get('specific_claim', 'N/A'),
                sentiment=prediction.get('sentiment', 'N/A'),
                timeframe=prediction.get('timeframe', 'N/A'),
                confidence_level=prediction.get('confidence_level', 'N/A'),
                prediction_logic=json.dumps(prediction.get('prediction_logic', {}), ensure_ascii=False),
                results_summary=json.dumps(results_summary, ensure_ascii=False, indent=2)
            )

            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": analysis_prompt}],
                response_format={"type": "json_object"}
            )
            
            result_text = response.choices[0].message.content
            analysis_result = json.loads(result_text)
            analysis_result['raw_response'] = result_text
            
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"æ·±åº¦åˆ†æsearch resultså¤±è´¥: {e}")
            return {"analysis": f"åˆ†æå¤±è´¥: {e}", "insights": [], "supporting_evidence": []}

    async def _comprehensive_prediction_analysis(self, prediction: Dict, request_results: List[Dict], 
                                               results_analysis: Dict, real_verification: Dict) -> Dict:
        """ç»¼åˆé¢„æµ‹åˆ†æ - ä½¿ç”¨å¢å¼ºçš„promptï¼Œé‡ç‚¹è§£å†³æ¨æ–‡è´¨é‡è¯„ä¼°é—®é¢˜"""
        try:
            self._rate_limit_delay()
            
            # ä½¿ç”¨å¢å¼ºçš„promptæ¨¡æ¿ - è¿™æ˜¯ä¿®å¤é—®é¢˜6çš„å…³é”®
            comprehensive_prompt = get_prompt_template(
                'comprehensive_analysis',
                original_prediction=json.dumps(prediction, ensure_ascii=False, indent=2, default=str),
                search_analysis=json.dumps(results_analysis, ensure_ascii=False, indent=2, default=str),
                verification_results=json.dumps(real_verification, ensure_ascii=False, indent=2, default=str)
            )

            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": comprehensive_prompt}],
                response_format={"type": "json_object"}
            )
            
            result_text = response.choices[0].message.content
            comprehensive_result = json.loads(result_text)
            comprehensive_result['raw_response'] = result_text
            
            # ç¡®ä¿åŒ…å«æ¨æ–‡è´¨é‡æ·±åº¦è¯„ä¼° - è¿™æ˜¯é—®é¢˜6çš„æ ¸å¿ƒä¿®å¤
            if 'ğŸ¯ TWEET_QUALITY_DEEP_EVALUATION' in comprehensive_result:
                tweet_quality = comprehensive_result['ğŸ¯ TWEET_QUALITY_DEEP_EVALUATION']
                self.logger.info(f"  ğŸ“Š æ¨æ–‡è´¨é‡è¯„ä¼°: {tweet_quality.get('æ¨æ–‡è´¨é‡åˆ¤æ–­', 'N/A')}")
                self.logger.info(f"  ğŸ“Š ç»¼åˆè¯„åˆ†: {tweet_quality.get('ç»¼åˆè¯„åˆ†', 'N/A')}")
                
                # è®°å½•è¯¦ç»†è¯„ä¼°ç»´åº¦
                content_score = tweet_quality.get('content_quality_score', 0)
                prediction_score = tweet_quality.get('prediction_value_score', 0)
                responsibility_score = tweet_quality.get('kol_responsibility_score', 0)
                impact_score = tweet_quality.get('market_impact_score', 0)
                
                self.logger.info(f"  ğŸ” è¯¦ç»†è¯„åˆ†: å†…å®¹è´¨é‡{content_score}, é¢„æµ‹ä»·å€¼{prediction_score}, KOLè´£ä»»{responsibility_score}, å¸‚åœºå½±å“{impact_score}")
            
            # ç¡®ä¿åŒ…å«æœ€ç»ˆåˆ¤æ–­ - è§£å†³"è¿™æ¡æ¨æ–‡åˆ°åº•æ˜¯å¥½è¿˜æ˜¯åï¼Ÿ"çš„é—®é¢˜
            if 'ğŸ¯ FINAL_VERDICT' in comprehensive_result:
                final_verdict = comprehensive_result['ğŸ¯ FINAL_VERDICT']
                judgment = final_verdict.get('æ¨æ–‡æ€»ä½“åˆ¤æ–­', 'N/A')
                self.logger.info(f"  ğŸ† æœ€ç»ˆåˆ¤æ–­: {judgment}")
            
            return comprehensive_result
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆé¢„æµ‹åˆ†æå¤±è´¥: {e}")
            return {"analysis": f"ç»¼åˆåˆ†æå¤±è´¥: {e}", "final_assessment": {"overall_accuracy": "unknown"}}

    def analyze_short_term_with_comprehensive_insights(self, short_predictions: List[Dict], long_predictions: List[Dict]) -> Dict:
        """é˜¶æ®µ1ï¼šå¢å¼ºç‰ˆçŸ­æœŸé¢„æµ‹åˆ†æ - ä½¿ç”¨å¢å¼ºçš„prompt"""
        try:
            self._rate_limit_delay()
            
            # æ„å»ºé•¿æœŸé¢„æµ‹èƒŒæ™¯
            long_context = []
            for pred in long_predictions:
                long_context.append(f"é•¿æœŸé¢„æµ‹: {pred['timeframe']} {pred['sentiment']} - {pred['specific_claim'][:100]}")
            long_context_text = "\n".join(long_context) if long_context else "æ— é•¿æœŸé¢„æµ‹"
            
            # æ„å»ºçŸ­æœŸé¢„æµ‹è¯¦ç»†æ•°æ®
            short_analysis_data = []
            for pred in short_predictions:
                real_verification = pred.get('real_verification', {})
                results_analysis = pred.get('results_analysis', {})
                comprehensive_analysis = pred.get('comprehensive_analysis', {})
                
                pred_data = {
                    'prediction': pred,
                    'request_summary': self._detailed_request_summary(pred['request_results']),
                    'real_verification': real_verification,
                    'results_analysis_summary': results_analysis.get('analysis_summary', 'N/A'),
                    'comprehensive_insights': comprehensive_analysis.get('comprehensive_summary', 'N/A'),
                    'final_assessment': comprehensive_analysis.get('final_assessment', {}),
                    'verification_accuracy': real_verification.get('overall_accuracy', 0),
                    # æ·»åŠ æ¨æ–‡è´¨é‡è¯„ä¼°æ•°æ®
                    'tweet_quality_evaluation': comprehensive_analysis.get('ğŸ¯ TWEET_QUALITY_DEEP_EVALUATION', {}),
                    'final_verdict': comprehensive_analysis.get('ğŸ¯ FINAL_VERDICT', {}),
                    'actionable_recommendations': comprehensive_analysis.get('ğŸš€ ACTIONABLE_RECOMMENDATIONS', {})
                }
                short_analysis_data.append(pred_data)
            
            # ä½¿ç”¨å¢å¼ºçš„promptæ¨¡æ¿
            short_term_prompt = get_prompt_template(
                'short_term_analysis',
                long_context=long_context_text,
                short_data=json.dumps(short_analysis_data, ensure_ascii=False, indent=2, default=str)
            )

            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": short_term_prompt}],
                response_format={"type": "json_object"}
            )
            
            result_text = response.choices[0].message.content
            short_analysis = json.loads(result_text)
            short_analysis['raw_response'] = result_text
            short_analysis['analysis_type'] = 'enhanced_short_term_analysis'
            
            avg_accuracy = short_analysis.get('short_term_aggregate_analysis', {}).get('overall_performance', {}).get('average_accuracy', 'N/A')
            self.logger.info(f"âœ… å¢å¼ºç‰ˆçŸ­æœŸé¢„æµ‹åˆ†æå®Œæˆ: å¹³å‡å‡†ç¡®ç‡{avg_accuracy}%")
            
            # è®°å½•æŠ•èµ„ä»·å€¼è¯„ä¼°
            investment_utility = short_analysis.get('short_term_aggregate_analysis', {}).get('ğŸ¯ INVESTMENT_UTILITY_SUMMARY', {})
            if investment_utility:
                overall_value = investment_utility.get('æ•´ä½“æŠ•èµ„ä»·å€¼', 'N/A')
                suitable_investors = investment_utility.get('æœ€é€‚åˆçš„æŠ•èµ„è€…ç±»å‹', 'N/A')
                self.logger.info(f"  ğŸ“Š æŠ•èµ„ä»·å€¼: {overall_value}, é€‚åˆæŠ•èµ„è€…: {suitable_investors}")
            
            return short_analysis
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºç‰ˆçŸ­æœŸé¢„æµ‹åˆ†æå‡ºé”™: {e}")
            return {
                "short_term_evaluations": [],
                "short_term_aggregate_analysis": {"overall_performance": {"average_accuracy": 50}},
                "analysis_type": "enhanced_short_term_analysis",
                "error": "å¢å¼ºç‰ˆçŸ­æœŸåˆ†æå¤±è´¥"
            }

    def analyze_long_term_with_strategic_depth(self, long_predictions: List[Dict], short_analysis: Dict) -> Dict:
        """é˜¶æ®µ2ï¼šå¢å¼ºç‰ˆé•¿æœŸé¢„æµ‹åˆ†æ - ä½¿ç”¨å¢å¼ºçš„prompt"""
        try:
            self._rate_limit_delay()
            
            # æ„å»ºé•¿æœŸé¢„æµ‹è¯¦ç»†æ•°æ®
            long_analysis_data = []
            for pred in long_predictions:
                real_verification = pred.get('real_verification', {})
                results_analysis = pred.get('results_analysis', {})
                comprehensive_analysis = pred.get('comprehensive_analysis', {})
                
                pred_data = {
                    'prediction': pred,
                    'request_summary': self._detailed_request_summary(pred['request_results']),
                    'real_verification': real_verification,
                    'results_analysis_summary': results_analysis.get('analysis_summary', 'N/A'),
                    'comprehensive_insights': comprehensive_analysis.get('comprehensive_summary', 'N/A'),
                    'final_assessment': comprehensive_analysis.get('final_assessment', {}),
                    'verification_accuracy': real_verification.get('overall_accuracy', 0),
                    # æ·»åŠ æ¨æ–‡è´¨é‡è¯„ä¼°æ•°æ®
                    'tweet_quality_evaluation': comprehensive_analysis.get('ğŸ¯ TWEET_QUALITY_DEEP_EVALUATION', {}),
                    'final_verdict': comprehensive_analysis.get('ğŸ¯ FINAL_VERDICT', {}),
                    'actionable_recommendations': comprehensive_analysis.get('ğŸš€ ACTIONABLE_RECOMMENDATIONS', {})
                }
                long_analysis_data.append(pred_data)
            
            # ä½¿ç”¨å¢å¼ºçš„promptæ¨¡æ¿
            long_term_prompt = get_prompt_template(
                'long_term_analysis',
                short_analysis=json.dumps(short_analysis, ensure_ascii=False, indent=2, default=str),
                long_data=json.dumps(long_analysis_data, ensure_ascii=False, indent=2, default=str)
            )

            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": long_term_prompt}],
                response_format={"type": "json_object"}
            )
            
            result_text = response.choices[0].message.content
            long_analysis = json.loads(result_text)
            long_analysis['raw_response'] = result_text
            long_analysis['analysis_type'] = 'enhanced_long_term_analysis'
            
            avg_accuracy = long_analysis.get('long_term_strategic_analysis', {}).get('overall_strategic_capability', {}).get('average_accuracy', 'N/A')
            self.logger.info(f"âœ… å¢å¼ºç‰ˆé•¿æœŸé¢„æµ‹åˆ†æå®Œæˆ: æˆ˜ç•¥èƒ½åŠ›è¯„åˆ†{avg_accuracy}")
            
            # è®°å½•æŠ•èµ„ç»„åˆä»·å€¼è¯„ä¼°
            strategic_merit = long_analysis.get('long_term_strategic_analysis', {}).get('ğŸ¯ STRATEGIC_INVESTMENT_MERIT', {})
            if strategic_merit:
                strategic_value = strategic_merit.get('æ•´ä½“æˆ˜ç•¥ä»·å€¼', 'N/A')
                investment_type = strategic_merit.get('æœ€é€‚åˆçš„æŠ•èµ„ç±»å‹', 'N/A')
                portfolio_role = strategic_merit.get('æŠ•èµ„ç»„åˆè§’è‰²', 'N/A')
                self.logger.info(f"  ğŸ“Š æˆ˜ç•¥ä»·å€¼: {strategic_value}, æŠ•èµ„ç±»å‹: {investment_type}, ç»„åˆè§’è‰²: {portfolio_role}")
            
            return long_analysis
            
        except Exception as e:
            self.logger.error(f"å¢å¼ºç‰ˆé•¿æœŸé¢„æµ‹åˆ†æå‡ºé”™: {e}")
            return {
                "long_term_evaluations": [],
                "long_term_strategic_analysis": {"overall_strategic_capability": {"average_accuracy": 50}},
                "analysis_type": "enhanced_long_term_analysis",
                "error": "å¢å¼ºç‰ˆé•¿æœŸåˆ†æå¤±è´¥"
            }

    def final_professional_kol_evaluation(self, short_analysis: Dict, long_analysis: Dict, 
                            chain_context: Dict, kol_name: str, coin_name: str) -> Dict:
        """é˜¶æ®µ3ï¼šç”Ÿæˆä¸“ä¸šçº§KOLç»¼åˆè¯„ä¼°æŠ¥å‘Š - ä¿®å¤è¯„åˆ†é€»è¾‘ï¼Œä½¿ç”¨å¢å¼ºçš„prompt"""
        try:
            self._rate_limit_delay()
            
            # è®¡ç®—å®é™…å‡†ç¡®ç‡ - æ·»åŠ å®‰å…¨å¤„ç†
            short_perf = short_analysis.get('short_term_aggregate_analysis', {}).get('overall_performance', {})
            long_perf = long_analysis.get('long_term_strategic_analysis', {}).get('overall_strategic_capability', {})
            
            # å®‰å…¨è·å–å‡†ç¡®ç‡å€¼ï¼Œç¡®ä¿ä¸æ˜¯ None
            short_accuracy = short_perf.get('average_accuracy')
            long_accuracy = long_perf.get('average_accuracy')
            
            # å¤„ç† None å€¼
            if short_accuracy is None:
                short_accuracy = 0
            if long_accuracy is None:
                long_accuracy = 0
            
            # ç¡®ä¿æ˜¯æ•°å­—ç±»å‹
            try:
                short_accuracy = float(short_accuracy)
            except (TypeError, ValueError):
                short_accuracy = 0
                
            try:
                long_accuracy = float(long_accuracy)
            except (TypeError, ValueError):
                long_accuracy = 0
            
            # ä¿®å¤è¯„åˆ†è®¡ç®—é€»è¾‘
            if short_accuracy > 0 and long_accuracy > 0:
                integrated_accuracy = (short_accuracy + long_accuracy) / 2
            elif short_accuracy > 0:
                integrated_accuracy = short_accuracy
            elif long_accuracy > 0:
                integrated_accuracy = long_accuracy
            else:
                integrated_accuracy = 50  # é»˜è®¤å€¼
            
            # ä½¿ç”¨å¢å¼ºçš„promptæ¨¡æ¿ - è¿™æ˜¯ä¿®å¤é—®é¢˜6çš„å…³é”®éƒ¨åˆ†
            final_prompt = get_prompt_template(
                'final_kol_evaluation',
                kol_name=kol_name,
                coin_name=coin_name,
                short_accuracy=short_accuracy,
                long_accuracy=long_accuracy,
                integrated_accuracy=integrated_accuracy,
                calculated_score=min(100, max(30, int(integrated_accuracy * 0.8 + 20))),
                chain_context=json.dumps(chain_context, ensure_ascii=False, indent=2, default=str),
                short_analysis=json.dumps(short_analysis, ensure_ascii=False, indent=2, default=str),
                long_analysis=json.dumps(long_analysis, ensure_ascii=False, indent=2, default=str),
                short_accuracy_val=short_accuracy,
                long_accuracy_val=long_accuracy,
                integrated_accuracy_val=integrated_accuracy,
                tech_score=min(100, max(30, int(integrated_accuracy * 0.9 + 10))),
                fund_score=min(100, max(30, int(integrated_accuracy * 0.85 + 15))),
                psych_score=min(100, max(40, int(integrated_accuracy * 0.95 + 5))),
                risk_score=min(100, max(35, int(integrated_accuracy * 0.75 + 25))),
                comm_score=min(100, max(50, int(integrated_accuracy * 0.8 + 20))),
                win_rate=min(100, max(30, int(integrated_accuracy)))
            )

            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": final_prompt}],
                response_format={"type": "json_object"},
            )
            
            result_text = response.choices[0].message.content
            final_evaluation = json.loads(result_text)
            final_evaluation['raw_response'] = result_text
            final_evaluation['analysis_type'] = 'professional_kol_evaluation'
            final_evaluation['kol_name'] = kol_name
            final_evaluation['coin_name'] = coin_name
            
            # ç¡®ä¿åŒ…å«å¿…è¦çš„æ•°æ®ç»“æ„
            if 'comprehensive_verification_analysis' not in final_evaluation:
                final_evaluation['comprehensive_verification_analysis'] = {
                    "short_term_performance": {"avg_accuracy": short_accuracy},
                    "long_term_performance": {"avg_accuracy": long_accuracy},
                    "integrated_performance": {"overall_accuracy": integrated_accuracy}
                }
            
            overall_grade = final_evaluation.get('executive_summary', {}).get('overall_grade', 'N/A')
            overall_score = final_evaluation.get('executive_summary', {}).get('overall_score', 'N/A')
            tier = final_evaluation.get('tier', None) or 'N/A'

            self.logger.info(f"âœ… ä¸“ä¸šçº§KOLæœ€ç»ˆè¯„ä¼°å®Œæˆ: ç­‰çº§ {tier} | è¯„çº§ {overall_grade}")
            
            # è®°å½•æ ¸å¿ƒè¯„ä¼°ç»“æœ - è¿™æ˜¯é—®é¢˜6çš„å…³é”®ä¿®å¤
            core_thesis = final_evaluation.get('ğŸ¯ CORE_INVESTMENT_THESIS', {})
            if core_thesis:
                attention_index = core_thesis.get('å€¼å¾—å…³æ³¨æŒ‡æ•°', 'N/A')
                follow_index = core_thesis.get('å€¼å¾—è·ŸéšæŒ‡æ•°', 'N/A')
                risk_level = core_thesis.get('é£é™©è­¦ç¤ºçº§åˆ«', 'N/A')
                self.logger.info(f"  ğŸ“Š æ ¸å¿ƒè¯„ä¼°: å…³æ³¨æŒ‡æ•°{attention_index}, è·ŸéšæŒ‡æ•°{follow_index}, é£é™©çº§åˆ«{risk_level}")
            
            # è®°å½•æœ€ç»ˆå†³ç­–æ¡†æ¶
            decision_framework = final_evaluation.get('ğŸ¯ FINAL_DECISION_FRAMEWORK', {})
            if decision_framework:
                attention_value = decision_framework.get('å…³æ³¨ä»·å€¼', 'N/A')
                follow_condition = decision_framework.get('è·Ÿéšæ¡ä»¶', 'N/A')
                self.logger.info(f"  ğŸ¯ å†³ç­–æ¡†æ¶: {attention_value[:50]}... | è·Ÿéšæ¡ä»¶: {follow_condition[:50]}...")
            
            return final_evaluation
            
        except Exception as e:
            self.logger.error(f"ä¸“ä¸šçº§KOLæœ€ç»ˆè¯„ä¼°å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return {
                "executive_summary": {"overall_grade": "C", "overall_score": 60},
                "comprehensive_verification_analysis": {
                    "short_term_performance": {"avg_accuracy": short_accuracy if 'short_accuracy' in locals() else 0},
                    "long_term_performance": {"avg_accuracy": long_accuracy if 'long_accuracy' in locals() else 0},
                    "integrated_performance": {"overall_accuracy": integrated_accuracy if 'integrated_accuracy' in locals() else 50}
                },
                "analysis_type": "professional_kol_evaluation",
                "kol_name": kol_name, 
                "coin_name": coin_name, 
                "error": f"ä¸“ä¸šçº§æœ€ç»ˆè¯„ä¼°å¤±è´¥: {str(e)}"
            }

    def _detailed_request_summary(self, request_results: List[Dict]) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„requestç»“æœæ€»ç»“ - å¢å¼ºç‰ˆ"""
        if not request_results: 
            return "æ— requestæ‰§è¡Œç»“æœ"
        
        summary = []
        for result in request_results:
            request = result['request']
            status = result['status']
            query = request['query']
            purpose = request.get('purpose', 'N/A')
            
            if status == 'success':
                result_data = result['result']
                if request['type'] == 'coingecko_api':
                    results_count = len(result_data.get('results', []))
                    detailed_results = result_data.get('detailed_results', {})
                    
                    summary.append(f"âœ… CoinGecko APIæŸ¥è¯¢: {query[:80]}")
                    summary.append(f"   ç›®çš„: {purpose}")
                    summary.append(f"   ç»“æœ: {results_count}é¡¹æ•°æ®")
                    
                    # æ·»åŠ è¯¦ç»†ç»“æœä¿¡æ¯
                    if detailed_results:
                        if 'price_data' in detailed_results:
                            price_data = detailed_results['price_data']
                            summary.append(f"   ä»·æ ¼æ•°æ®: å½“å‰${price_data.get('current_price', 'N/A')}, 24hå˜åŒ–{price_data.get('price_change_percentage_24h', 0):.2f}%")
                        if 'technical_indicators' in detailed_results:
                            summary.append(f"   æŠ€æœ¯æŒ‡æ ‡: {', '.join(detailed_results['technical_indicators'][:2])}")
                        if 'market_data' in detailed_results:
                            summary.append(f"   å¸‚åœºæ•°æ®: {', '.join(detailed_results['market_data'][:2])}")
                    
                    # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
                    if not result_data.get('success', True):
                        summary.append(f"   é”™è¯¯åŸå› : {result_data.get('error', 'æœªçŸ¥é”™è¯¯')}")
                        summary.append(f"   è¯¦ç»†é”™è¯¯: {result_data.get('detailed_error', 'N/A')}")
                else:
                    # Webæœç´¢ç»“æœ - ç¡®ä¿å®Œæ•´å±•ç¤º
                    full_content = result_data.get('full_content', '')
                    if full_content:
                        content_preview = full_content[:200] + "..." if len(full_content) > 200 else full_content
                        summary.append(f"âœ… Webæœç´¢: {query[:80]}")
                        summary.append(f"   ç›®çš„: {purpose}")
                        summary.append(f"   ç»“æœ: {content_preview}")
                        summary.append(f"   å®Œæ•´å†…å®¹é•¿åº¦: {len(full_content)}å­—ç¬¦")
                    else:
                        search_summary = result_data.get('summary', 'N/A')[:150]
                        summary.append(f"âœ… Webæœç´¢: {query[:80]}")
                        summary.append(f"   ç›®çš„: {purpose}")
                        summary.append(f"   ç»“æœ: {search_summary}")
            else:
                error_msg = result['result'].get('error', 'Unknown error')
                summary.append(f"âŒ æŸ¥è¯¢å¤±è´¥: {query[:80]}")
                summary.append(f"   ç›®çš„: {purpose}")
                summary.append(f"   é”™è¯¯: {error_msg[:100]}")
        
        return "\n".join(summary)

    def assign_tiers(self, analysis_results: List[Dict]) -> Dict[str, int]:
        """æ ¹æ®ç»¼åˆåˆ†æ•°æŒ‰ç™¾åˆ†æ¯”åˆ†é…ç­‰çº§ï¼ˆTierï¼‰ã€‚
        ä¿®æ”¹æ¯ä¸ª analysis_result çš„ final_evaluationï¼Œä½¿ä¹‹åªå…¬å¼€ `tier` å­—æ®µï¼ˆæ•°å€¼åˆ†æ•°ç§»è‡³å†…éƒ¨å­—æ®µ `_internal_score`ï¼‰ã€‚
        è¿”å›åˆ†é…åçš„å„ç­‰çº§è®¡æ•°ç»Ÿè®¡ã€‚
        """
        # ç­‰çº§åŠå¯¹åº”ç™¾åˆ†æ¯”ï¼ˆæŒ‰é¢˜ä¸»è¦æ±‚çš„é¡ºåºï¼Œä»é«˜åˆ°ä½ï¼‰
        tiers = [
            ("S+", 2), ("S", 4), ("S-", 4), ("A+", 5), ("A", 7), ("A-", 8),
            ("B+", 12), ("B", 16), ("B-", 12), ("C+", 8), ("C", 7), ("C-", 5),
            ("D+", 4), ("D", 4), ("D-", 2)
        ]

        # æŠŠæœ‰æŠ¥å‘Šçš„é¡¹å’Œæ— æŠ¥å‘Šçš„é¡¹åˆ†å¼€ï¼šåªæœ‰æœ‰å®é™…è¯„ä¼°/é¢„æµ‹æˆ– final_evaluation éç©ºçš„ï¼Œæ‰å‚ä¸æ’ååˆ†é…ã€‚
        reportable = []
        non_reportable = []
        for r in analysis_results:
            fe = r.get('final_evaluation')
            has_predictions = bool(r.get('predictions') or r.get('prediction_results') or r.get('request_results'))
            has_fe_content = False
            if fe and isinstance(fe, dict) and any(k for k in fe.keys() if k not in ('_internal_score', '_internal_rank', 'tier')):
                has_fe_content = True

            if has_fe_content or has_predictions:
                reportable.append(r)
            else:
                non_reportable.append(r)

        m = len(reportable)
        # å¦‚æœæ²¡æœ‰å¯æ’åçš„é¡¹ï¼Œåˆ™å°†æ‰€æœ‰äººæ ‡ä¸ºæœ€ä½ç­‰çº§å¹¶è®¡æ•°
        tier_counts = {name: 0 for name, _ in tiers}
        if m == 0:
            for idx, r in enumerate(non_reportable):
                fe = r.setdefault('final_evaluation', {})
                fe['_internal_score'] = 0.0
                fe['_internal_rank'] = idx + 1
                fe['tier'] = tiers[-1][0]
                if 'executive_summary' in fe:
                    fe['executive_summary'].pop('overall_score', None)
                tier_counts[tiers[-1][0]] += 1
            return tier_counts

        # æå–æ¯ä¸ªå¯æ’åç»“æœçš„æ•°å€¼åˆ†æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ executive_summary.overall_scoreï¼Œå…¶æ¬¡ä½¿ç”¨ç»¼åˆaccuracyï¼‰ï¼Œç¼ºçœä¸º0
        # ä¸ºäº†é¿å…åŒä¸€ä½œè€…å ç”¨å¤šä¸ªåé¢ï¼šå…ˆæŒ‰ä½œè€…èšåˆï¼Œä½¿ç”¨ä½œè€…çš„æœ€é«˜åˆ†ä½œä¸ºä½œè€…åˆ†æ•°è¿›è¡Œæ’å
        author_map = {}  # author -> list of (r, score_val)
        for r in reportable:
            fe = r.get('final_evaluation', {}) or {}
            score = None
            score = fe.get('executive_summary', {}).get('overall_score') if isinstance(fe.get('executive_summary'), dict) else None
            if score is None:
                score = fe.get('comprehensive_verification_analysis', {}).get('integrated_performance', {}).get('overall_accuracy') if isinstance(fe.get('comprehensive_verification_analysis'), dict) else None
            try:
                score_val = float(score) if score is not None else 0.0
            except Exception:
                score_val = 0.0

            author = r.get('kol_name') or r.get('author_name') or r.get('final_evaluation', {}).get('kol_name') or r.get('chain_key')
            author = (author or 'unknown').strip().lower()
            author_map.setdefault(author, []).append((r, score_val))

        # æ„å»ºä½œè€…æœ€ä½³åˆ†åˆ—è¡¨ï¼Œç”¨äºæ’å
        author_best = []  # list of (author, best_score, representative_r)
        for author, items in author_map.items():
            # é€‰æ‹©è¯¥ä½œè€…æœ€é«˜åˆ†çš„ä¸€æ¡ä½œä¸ºä»£è¡¨
            items_sorted = sorted(items, key=lambda x: x[1], reverse=True)
            best_score = items_sorted[0][1]
            rep_r = items_sorted[0][0]
            author_best.append((author, best_score, rep_r))

        # æŒ‰ä½œè€…æœ€é«˜åˆ†é™åºæ’åº
        author_best.sort(key=lambda x: x[1], reverse=True)

        # è®¡ç®—æ¯ä¸€ç­‰çº§åº”åˆ†é…çš„äººæ•°ï¼ˆåŸºäºå¯æ’åçš„å”¯ä¸€ä½œè€…æ•°é‡ m_uniqueï¼‰
        m_unique = len(author_best)
        counts = [int((pct * m_unique) / 100) for _, pct in tiers]

        assigned = sum(counts)
        remaining = m_unique - assigned
        idx = 0
        # å°†å‰©ä½™åé¢ä»æœ€é«˜ç­‰çº§ä¾æ¬¡åˆ†é…
        while remaining > 0:
            counts[idx] += 1
            remaining -= 1
            idx = (idx + 1) % len(counts)

        # æŒ‰ä½œè€…åˆ†é…ç­‰çº§ï¼šæ¯ä½ä½œè€…æœ€å¤šè·å¾—ä¸€ä¸ªåé¢ï¼ˆä¸”å ç”¨ä¸€ä¸ªæ’åä½ç½®ï¼‰ã€‚åŒä¸€ä½œè€…çš„æ‰€æœ‰æ¡ç›®å°†è¢«æ ‡è®°ä¸ºç›¸åŒç­‰çº§ã€‚
        # æ–°å¢è§„åˆ™ï¼šä¸ºè‹¥å¹²é«˜ç­‰çº§è®¾ç½®é¢å¤–é—¨æ§›ï¼ˆæœ€å°æœ‰æ•ˆæ¨æ–‡æ•°ã€æœ€å°é¢„æµ‹å¸ç§æ•°ï¼‰ï¼Œåªæœ‰æ»¡è¶³é—¨æ§›çš„ä½œè€…æ‰æœ‰èµ„æ ¼è¢«åˆ†é…åˆ°è¯¥ç­‰çº§ã€‚
        thresholds = {
            'S+': (10, 3),
            'S': (10, 3),
            'S-': (10, 3),
            'A+': (5, 5),
            'A': (5, 2),
            'A-': (2, 2),
            'B+': (2, 0),
            # å…¶ä»–ç­‰çº§æ— é¢å¤–é—¨æ§›
        }

        def author_meets_threshold(author_items: List[Tuple[Dict, float]], tier_name: str) -> bool:
            """åˆ¤æ–­ä½œè€…æ˜¯å¦æ»¡è¶³æŒ‡å®šç­‰çº§çš„é—¨æ§›ã€‚
            author_items: list of (r, score_val)
            è¿”å› True/False
            """
            min_preds, min_coins = thresholds.get(tier_name, (0, 0))
            # ç»Ÿè®¡æœ‰æ•ˆæ¨æ–‡æ•°ï¼ˆä»¥prediction_resultsæˆ–predictionsä¸ºå‡†ï¼‰
            total_preds = 0
            coins = set()
            for r_item, _ in author_items:
                preds = r_item.get('prediction_results') or r_item.get('predictions') or []
                try:
                    total_preds += len(preds)
                except Exception:
                    # å¦‚æœ preds ä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•å½“ä½œå•ä¸ªé¡¹è®¡æ•°
                    if preds:
                        total_preds += 1
                coin_name = (r_item.get('coin_name') or r_item.get('coin', '') or '').strip().lower()
                if coin_name:
                    coins.add(coin_name)
            distinct_coins = len(coins)
            return (total_preds >= min_preds) and (distinct_coins >= min_coins)

        remaining_authors = list(author_best)  # mutable copy
        remaining_map = {a[0]: a for a in remaining_authors}
        i = 0
        assigned_authors = set()

        for (tier_name, _), cnt in zip(tiers, counts):
            if cnt <= 0:
                continue
            # ä»å‰©ä½™ä½œè€…ä¸­ç­›é€‰æ»¡è¶³è¯¥ç­‰çº§é—¨æ§›çš„å€™é€‰äººï¼ŒæŒ‰ä½œè€…æœ€é«˜åˆ†æ’åº
            candidates = [a for a in remaining_authors if a[0] not in assigned_authors]
            # ä¾æ®é—¨æ§›è¿‡æ»¤
            eligible = []
            for author, best_score, rep_r in candidates:
                items = author_map.get(author, [])
                if author_meets_threshold(items, tier_name):
                    eligible.append((author, best_score, rep_r))

            # æŒ‰åˆ†æ•°é™åºé€‰å‡ºæœ¬ç­‰çº§çš„åé¢
            eligible.sort(key=lambda x: x[1], reverse=True)
            selected = eligible[:cnt]

            for author, best_score, rep_r in selected:
                items = author_map.get(author, [])
                for r_item, score_val in items:
                    fe = r_item.setdefault('final_evaluation', {})
                    fe['_internal_score'] = score_val
                    fe['_internal_rank'] = i + 1
                    fe['tier'] = tier_name
                    if 'executive_summary' in fe and isinstance(fe['executive_summary'], dict):
                        fe['executive_summary'].pop('overall_score', None)
                tier_counts[tier_name] += 1
                assigned_authors.add(author)
                i += 1

            # ä» remaining_authors ä¸­ç§»é™¤å·²åˆ†é…çš„ä½œè€…
            if selected:
                sel_set = set(a[0] for a in selected)
                remaining_authors = [a for a in remaining_authors if a[0] not in sel_set]

        # æ‰€æœ‰æœªè¢«åˆ†é…åˆ°ç­‰çº§çš„ä½œè€…ï¼Œç»Ÿä¸€æ ‡ä¸ºæœ€ä½ç­‰çº§
        lowest_tier = tiers[-1][0]
        for author, best_score, rep_r in remaining_authors:
            if author in assigned_authors:
                continue
            items = author_map.get(author, [])
            for r_item, score_val in items:
                fe = r_item.setdefault('final_evaluation', {})
                fe['_internal_score'] = score_val
                fe['_internal_rank'] = i + 1
                fe['tier'] = lowest_tier
                if 'executive_summary' in fe and isinstance(fe['executive_summary'], dict):
                    fe['executive_summary'].pop('overall_score', None)
            tier_counts[lowest_tier] += 1
            i += 1

        # å°†æ— æŠ¥å‘Šçš„é¡¹æ”¾åˆ°æœ«å°¾ï¼Œæ ‡ä¸ºæœ€ä½ç­‰çº§ï¼ˆä¾‹å¦‚ D-ï¼‰ï¼Œå¹¶æŠŠå®ƒä»¬è®¡å…¥ç»Ÿè®¡
        for j, r in enumerate(non_reportable):
            fe = r.setdefault('final_evaluation', {})
            fe['_internal_score'] = 0.0
            fe['_internal_rank'] = m_unique + j + 1
            fe['tier'] = tiers[-1][0]
            if 'executive_summary' in fe and isinstance(fe['executive_summary'], dict):
                fe['executive_summary'].pop('overall_score', None)
            tier_counts[tiers[-1][0]] += 1

        return tier_counts

    # ========================================================================
    # å›¾è¡¨ç”Ÿæˆ - ä¼˜åŒ–ç‰ˆï¼Œå»æ‰é¢„æµ‹ç‚¹ç”Ÿæˆï¼Œé…åˆHTML hotpointç³»ç»Ÿ
    # ========================================================================
    
    def get_crypto_price_history(self, coin_id: str, start_date: datetime, end_date: datetime) -> Optional[Dict]:
        """ä»CoinGeckoè·å–ä»·æ ¼å†å²æ•°æ® - ä¿®å¤ç‰ˆ"""
        self._rate_limit_delay()
        
        try:
            start_timestamp = int(start_date.timestamp())
            end_timestamp = int(end_date.timestamp())
            
            # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å‚æ•° - ä¿®å¤422é”™è¯¯
            if self.coingecko_api_key:
                url = "https://pro-api.coingecko.com/api/v3/coins/{}/market_chart/range".format(coin_id)
                headers = {"x-cg-pro-api-key": self.coingecko_api_key}
            else:
                url = "https://api.coingecko.com/api/v3/coins/{}/market_chart/range".format(coin_id)
                headers = {}
            
            params = {
                'vs_currency': 'usd', 
                'from': start_timestamp, 
                'to': end_timestamp
            }
            
            self.logger.info(f"è·å– {coin_id} ä»·æ ¼æ•°æ®: {start_date.date()} åˆ° {end_date.date()}")
            self.logger.debug(f"è¯·æ±‚å‚æ•°: {params}")
            
            response = requests.get(url, params=params, headers=headers, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                prices = data.get('prices', [])
                if not prices: 
                    self.logger.warning(f"æœªè·å–åˆ° {coin_id} çš„ä»·æ ¼æ•°æ®")
                    return None
                
                df = pd.DataFrame(prices, columns=['timestamp', 'price'])
                df['date'] = pd.to_datetime(df['timestamp'], unit='ms')
                self.logger.info(f"æˆåŠŸè·å– {coin_id} ä»·æ ¼æ•°æ®ï¼Œå…± {len(df)} ä¸ªæ•°æ®ç‚¹")
                return {'coin_id': coin_id, 'data': df, 'start_date': start_date, 'end_date': end_date}
            
            elif response.status_code == 422:
                self.logger.error(f"422é”™è¯¯ - APIå‚æ•°æ— æ•ˆ: {response.text}")
                self.logger.error(f"è¯·æ±‚URL: {url}")
                self.logger.error(f"å‚æ•°: {params}")
                return None
            
            elif response.status_code == 429:
                self.logger.warning("APIé™æµï¼Œç­‰å¾…60ç§’...")
                time.sleep(60)
                return self.get_crypto_price_history(coin_id, start_date, end_date)
            
            elif response.status_code == 404:
                self.logger.warning(f"æœªæ‰¾åˆ°å¸ç§: {coin_id}")
                return None
            
            else:
                self.logger.error(f"è·å–ä»·æ ¼æ•°æ®å¤±è´¥: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            self.logger.error(f"è·å–ä»·æ ¼æ•°æ®æ—¶å‡ºé”™: {e}")
            return None

    def create_optimized_chart_for_html(self, kol_name: str, coin_name: str, 
                                       prediction_results: List[Dict], 
                                       price_data: Dict) -> Optional[Dict]:
        """åˆ›å»ºä¼˜åŒ–çš„å›¾è¡¨ï¼Œé…åˆHTML hotpointç³»ç»Ÿ - å»æ‰é¢„æµ‹ç‚¹ç»˜åˆ¶"""
        try:
            plt.rcParams['font.sans-serif'] = ['Noto Serif CJK SC', 'Noto Serif CJK SC Regular', 'Liberation Sans']
            plt.rcParams['axes.unicode_minus'] = False
            
            df = price_data['data']
            fig, ax = plt.subplots(figsize=(24, 14), dpi=120)
            
            # ä»…ç»˜åˆ¶ä»·æ ¼æ›²çº¿ï¼Œä¸ç»˜åˆ¶é¢„æµ‹ç‚¹
            ax.plot(df['date'], df['price'], linewidth=3, color='#1f77b4', 
                   label=f'{coin_name} Price', alpha=0.9, zorder=1)
            
            # é¢„æµ‹æ•°æ®å¤„ç† - ä¸ºHTMLç”Ÿæˆåæ ‡ä¿¡æ¯
            prediction_coordinates = []
            max_prediction_date = None
            
            for i, prediction in enumerate(prediction_results):
                if 'tweet_info' in prediction:
                    tweet_info = prediction['tweet_info']
                    pred_date = pd.to_datetime(tweet_info['tweet_created_at'])
                    
                    # è·å–æœ€é•¿çš„é¢„æµ‹æ—¶é—´ç‚¹
                    check_points = prediction.get('intelligent_check_points', [])
                    for check_point in check_points:
                        # è®¡ç®—æ£€æŸ¥ç‚¹çš„æ—¶é—´
                        check_timestamp = self.verification_engine.calculate_target_timestamp(
                            int(pred_date.timestamp()), check_point
                        )
                        check_date = datetime.fromtimestamp(check_timestamp)
                        if max_prediction_date is None or check_date > max_prediction_date:
                            max_prediction_date = check_date
                    
                    # æ‰¾åˆ°æœ€æ¥è¿‘çš„ä»·æ ¼ç‚¹
                    closest_idx = (df['date'] - pred_date).abs().idxmin()
                    pred_price = df.loc[closest_idx, 'price']
                    
                    # è·å–åˆ†ææ•°æ®
                    sentiment = prediction.get('sentiment', 'bullish')
                    timeframe = prediction.get('timeframe', 'short_term')
                    strength = prediction.get('strength', 'moderate')
                    
                    real_verification = prediction.get('real_verification', {})
                    verification_accuracy = real_verification.get('overall_accuracy', 0)
                    
                    comprehensive_analysis = prediction.get('comprehensive_analysis', {})
                    
                    # è®¡ç®—ç›¸å¯¹åæ ‡ - è¿™æ˜¯å…³é”®ï¼ŒHTML hotpointéœ€è¦ç›¸å¯¹ä½ç½®
                    date_range_sec = (df['date'].max() - df['date'].min()).total_seconds()
                    relative_x = ((pred_date - df['date'].min()).total_seconds() / date_range_sec) if date_range_sec > 0 else 0.5
                    price_range_val = df['price'].max() - df['price'].min()
                    relative_y = ((pred_price - df['price'].min()) / price_range_val) if price_range_val > 0 else 0.5
                    
                    # æ—¶é—´æ¡†æ¶æ˜ å°„
                    timeframe_map = {'short_term': 'ST', 'medium_term': 'MT', 'long_term': 'LT'}
                    sentiment_map = {'bullish': 'â†—', 'bearish': 'â†˜', 'neutral': 'â†’'}
                    
                    # åŸºäºæ¨æ–‡è´¨é‡è¯„ä¼°çš„çŠ¶æ€ç¬¦å·
                    tweet_quality = comprehensive_analysis.get('ğŸ¯ TWEET_QUALITY_DEEP_EVALUATION', {})
                    final_verdict = comprehensive_analysis.get('ğŸ¯ FINAL_VERDICT', {})
                    
                    overall_judgment = final_verdict.get('æ¨æ–‡æ€»ä½“åˆ¤æ–­', '')
                    tweet_quality_rating = tweet_quality.get('æ¨æ–‡è´¨é‡åˆ¤æ–­', '')
                    
                    if 'å¼ºçƒˆæ¨è' in overall_judgment or 'ä¼˜ç§€' in tweet_quality_rating:
                        status_symbol = 'â­'
                    elif 'æ¨è' in overall_judgment or 'è‰¯å¥½' in tweet_quality_rating:
                        status_symbol = 'âœ“'
                    elif 'è°¨æ…' in overall_judgment or 'ä¸€èˆ¬' in tweet_quality_rating:
                        status_symbol = '~'
                    elif 'ä¸æ¨è' in overall_judgment or 'è¾ƒå·®' in tweet_quality_rating:
                        status_symbol = 'âœ—'
                    else:
                        # åå¤‡æ–¹æ¡ˆï¼šåŸºäºéªŒè¯å‡†ç¡®ç‡
                        if verification_accuracy >= 80:
                            status_symbol = 'âœ“'
                        elif verification_accuracy >= 60:
                            status_symbol = '~'
                        else:
                            status_symbol = 'âœ—'
                    
                    label_text = f"{timeframe_map.get(timeframe, 'ST')}{i+1}{sentiment_map.get(sentiment, 'â†—')}{status_symbol}({verification_accuracy:.0f}%)"
                    
                    # æ„å»ºè¯¦ç»†åæ ‡ä¿¡æ¯ - ä¾›HTMLä½¿ç”¨
                    prediction_coordinates.append({
                        'index': i, 
                        'label': label_text, 
                        'date': pred_date.strftime('%Y-%m-%d %H:%M'), 
                        'price': f"${pred_price:.4f}", 
                        'display_price': f"${pred_price:.4f}",
                        'sentiment': sentiment, 
                        'timeframe': timeframe, 
                        'strength': strength, 
                        'tweet_id': prediction.get('tweet_info', {}).get('tweet_id', ''), 
                        'content': prediction.get('specific_claim', '')[:200] + '...',
                        'real_verification': real_verification, 
                        'verification_accuracy': verification_accuracy, 
                        'comprehensive_analysis': comprehensive_analysis, 
                        'tweet_quality_evaluation': tweet_quality,
                        'final_verdict': final_verdict,
                        'intelligent_check_points': prediction.get('intelligent_check_points', []), 
                        'time_selection_reasoning': prediction.get('time_selection_reasoning', ''),
                        'prediction_logic': prediction.get('prediction_logic', {}),
                        'relative_x': max(0.05, min(0.95, relative_x)), 
                        'relative_y': max(0.1, min(0.9, relative_y)), 
                        'data_x': pred_date.timestamp() * 1000, 
                        'data_y': float(pred_price), 
                        'color': self._get_color_by_quality(tweet_quality, verification_accuracy),
                        'marker': {'bullish': '^', 'bearish': 'v', 'neutral': 'o'}.get(sentiment, '^')
                    })
            
            # å¦‚æœéœ€è¦ï¼Œæ‰©å±•ä»·æ ¼æ•°æ®åˆ°æœ€é•¿é¢„æµ‹ç‚¹å7å¤©
            if max_prediction_date and max_prediction_date > df['date'].max():
                target_end_date = max_prediction_date + timedelta(days=7)
                self.logger.info(f"æ‰©å±•å›¾è¡¨æ—¶é—´èŒƒå›´åˆ°: {target_end_date.strftime('%Y-%m-%d')}")
                
                # è·å–æ‰©å±•çš„ä»·æ ¼æ•°æ®
                extended_price_data = self.get_crypto_price_history(
                    price_data['coin_id'],
                    df['date'].min(),
                    target_end_date
                )
                
                if extended_price_data:
                    df = extended_price_data['data']
                    # é‡æ–°ç»˜åˆ¶ä»·æ ¼æ›²çº¿
                    ax.clear()
                    ax.plot(df['date'], df['price'], linewidth=3, color='#1f77b4', 
                           label=f'{coin_name} Price', alpha=0.9, zorder=1)
            
            # è®¾ç½®å›¾è¡¨æ ·å¼
            title_text = f'@{kol_name} Ã— {coin_name} ä¸“ä¸šçº§æ¨ç†é“¾åˆ†æ'
            ax.set_title(title_text, fontsize=20, fontweight='bold', pad=40)
            ax.set_xlabel('Date', fontsize=18)
            ax.set_ylabel('Price (USD)', fontsize=18)
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
            ax.xaxis.set_major_locator(mdates.DayLocator(interval=max(1, len(df)//20)))
            plt.xticks(rotation=45)
            ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:.4f}'))
            
            # ç®€åŒ–å›¾ä¾‹ - ä¸åŒ…å«é¢„æµ‹ç‚¹å›¾ä¾‹ï¼Œå› ä¸ºHTMLä¼šå¤„ç†
            legend_elements = [
                plt.Line2D([0], [0], color='#1f77b4', linewidth=3, label=f'{coin_name} ä»·æ ¼'),
            ]
            ax.legend(handles=legend_elements, loc='upper left', fontsize=16, 
                     frameon=True, fancybox=True, shadow=True)
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_filename = f"{kol_name}_{coin_name.replace('/', '_')}_optimized_for_html.png"
            chart_path = os.path.join(self.output_dir, "charts", chart_filename)
            plt.savefig(chart_path, format='png', dpi=120, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
            
            # ç”Ÿæˆbase64
            buffer = BytesIO()
            plt.savefig(buffer, format='png', dpi=120, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
            buffer.seek(0)
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            fig_width, fig_height = fig.get_size_inches() * fig.dpi
            plt.close(fig)
            
            self.logger.info(f"âœ… ä¼˜åŒ–å›¾è¡¨åˆ›å»ºå®Œæˆï¼ˆé…åˆHTML hotpointï¼‰: {chart_path}")
            self.logger.info(f"  ğŸ“Š å›¾è¡¨ä¿¡æ¯: {len(prediction_coordinates)}ä¸ªé¢„æµ‹ç‚¹ï¼Œå°ºå¯¸{fig_width:.0f}x{fig_height:.0f}")
            
            return {
                'image_base64': image_base64, 
                'prediction_coordinates': prediction_coordinates, 
                'chart_path': chart_path, 
                'chart_dimensions': {'width': float(fig_width), 'height': float(fig_height)}, 
                'version': 'optimized_for_html_hotpoint_system',
                'total_predictions': len(prediction_coordinates),
                'price_data_points': len(df),
                'date_range': {
                    'start': df['date'].min().isoformat(),
                    'end': df['date'].max().isoformat()
                }
            }
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºä¼˜åŒ–å›¾è¡¨æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _get_color_by_quality(self, tweet_quality: Dict, verification_accuracy: float) -> str:
        """æ ¹æ®æ¨æ–‡è´¨é‡è¯„ä¼°è·å–é¢œè‰²"""
        try:
            # é¦–å…ˆå°è¯•åŸºäºæ¨æ–‡è´¨é‡è¯„ä¼°
            quality_rating = tweet_quality.get('æ¨æ–‡è´¨é‡åˆ¤æ–­', '')
            
            if 'ä¼˜ç§€' in quality_rating:
                return '#2E7D32'  # æ·±ç»¿
            elif 'è‰¯å¥½' in quality_rating:
                return '#4CAF50'  # ç»¿è‰²
            elif 'ä¸€èˆ¬' in quality_rating:
                return '#FF9800'  # æ©™è‰²
            elif 'è¾ƒå·®' in quality_rating:
                return '#F44336'  # çº¢è‰²
            else:
                # åå¤‡æ–¹æ¡ˆï¼šåŸºäºéªŒè¯å‡†ç¡®ç‡
                if verification_accuracy >= 80:
                    return '#4CAF50'
                elif verification_accuracy >= 60:
                    return '#FF9800'
                else:
                    return '#F44336'
        except:
            # é»˜è®¤é¢œè‰²
            return '#FF9800'

    # ========================================================================
    # ä¸»åˆ†ææµç¨‹ - ä¼˜åŒ–ç‰ˆ
    # ========================================================================
    
    async def analyze_reasoning_chain_with_html_optimization(self, chain_key: str, reasoning_chain: List[Dict]) -> Dict:
        """åˆ†æå•ä¸ªæ¨ç†é“¾ï¼Œä¼˜åŒ–é…åˆHTMLç”Ÿæˆå™¨ - å®Œæ•´ä¼˜åŒ–ç‰ˆ"""
        try:
            # ä½¿ç”¨ä¸æ„å»ºæ—¶ä¸€è‡´çš„ä¸“ç”¨åˆ†éš”ç¬¦è§£æchain_key
            if '|||' in chain_key:
                kol_name, coin_name = chain_key.split('|||', 1)
            else:
                # å…¼å®¹æ—§çš„ä¸‹åˆ’çº¿åˆ†éš”æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                kol_name, coin_name = chain_key.split('_', 1)
            self.logger.info(f"ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆæ¨ç†é“¾åˆ†æ: @{kol_name} Ã— {coin_name} ({len(reasoning_chain)} æ¡é¢„æµ‹)")
            
            # Phase 1: é¢„å¤„ç†æ¨ç†é“¾èƒŒæ™¯ (å¢å¼ºç‰ˆ)
            self.logger.info("ğŸ“‹ Phase 1: è¯¦ç»†é¢„å¤„ç†æ¨ç†é“¾èƒŒæ™¯...")
            chain_context = self.preprocess_reasoning_chain(reasoning_chain, kol_name, coin_name)
            
            # Phase 2: ä¸“ä¸šçº§Super Analyzerå¹¶å‘åˆ†æ
            self.logger.info("ğŸ§  Phase 2: ä¸“ä¸šçº§Super Analyzerå¹¶å‘åˆ†æ...")
            all_predictions = await self.analyze_tweets_concurrently(reasoning_chain, chain_context, coin_name)
            
            if not all_predictions:
                self.logger.warning("  âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆé¢„æµ‹ï¼Œè·³è¿‡è¯¥æ¨ç†é“¾")
                return {'error': 'æœªæ‰¾åˆ°æœ‰æ•ˆé¢„æµ‹'}
            
            self.logger.info(f"  âœ… å…±æ‰¾åˆ° {len(all_predictions)} ä¸ªæœ‰æ•ˆé¢„æµ‹")
            
            # Phase 3: æ‰§è¡Œæ·±åº¦requests + çœŸå®éªŒè¯ - ä½¿ç”¨ä¿®å¤çš„å¼•æ“
            self.logger.info("ğŸ” Phase 3: æ‰§è¡Œæ·±åº¦search requests + çœŸå®éªŒè¯...")
            prediction_results = await self.execute_requests_with_deep_analysis(all_predictions)
            
            # Phase 4: æŒ‰æ—¶é—´æ¡†æ¶åˆ†ç»„
            self.logger.info("ğŸ“Š Phase 4: æŒ‰æ—¶é—´æ¡†æ¶åˆ†ç»„è¿›è¡Œä¸“ä¸šåˆ†æ...")
            short_predictions = [p for p in prediction_results if p.get('timeframe') == 'short_term']
            long_predictions = [p for p in prediction_results if p.get('timeframe') in ['medium_term', 'long_term']]
            
            self.logger.info(f"  çŸ­æœŸé¢„æµ‹: {len(short_predictions)} ä¸ª")
            self.logger.info(f"  é•¿æœŸé¢„æµ‹: {len(long_predictions)} ä¸ª")
            
            # Phase 5: å¢å¼ºç‰ˆçŸ­æœŸåˆ†æ
            self.logger.info("âš¡ Phase 5: å¢å¼ºç‰ˆçŸ­æœŸé¢„æµ‹åˆ†æ...")
            short_analysis = self.analyze_short_term_with_comprehensive_insights(short_predictions, long_predictions)
            
            # Phase 6: å¢å¼ºç‰ˆé•¿æœŸåˆ†æ
            self.logger.info("ğŸ¯ Phase 6: å¢å¼ºç‰ˆé•¿æœŸé¢„æµ‹åˆ†æ...")
            long_analysis = self.analyze_long_term_with_strategic_depth(long_predictions, short_analysis)
            
            # Phase 7: ä¸“ä¸šçº§æœ€ç»ˆKOLè¯„ä¼°
            self.logger.info("ğŸ† Phase 7: ä¸“ä¸šçº§KOLæœ€ç»ˆè¯„ä¼°...")
            final_evaluation = self.final_professional_kol_evaluation(short_analysis, long_analysis, chain_context, kol_name, coin_name)
            
            # Phase 8: ç”Ÿæˆä¼˜åŒ–å›¾è¡¨ - é…åˆHTML hotpointç³»ç»Ÿ
            self.logger.info("ğŸ“ˆ Phase 8: ç”Ÿæˆä¼˜åŒ–å›¾è¡¨ï¼ˆé…åˆHTML hotpointç³»ç»Ÿï¼‰...")
            chart_data = None
            if prediction_results and 'tweet_info' in prediction_results[0]:
                coingecko_id = prediction_results[0]['tweet_info'].get('coingecko_id')
                if coingecko_id:
                    dates = [pd.to_datetime(p['tweet_info']['tweet_created_at']) for p in prediction_results]
                    start_date, end_date = min(dates) - timedelta(days=7), max(dates) + timedelta(days=30)
                    price_data = self.get_crypto_price_history(coingecko_id, start_date, end_date)
                    if price_data:
                        chart_data = self.create_optimized_chart_for_html(kol_name, coin_name, prediction_results, price_data)

            # Phase 9: å‡†å¤‡æ•°æ®ç»“æ„ - ä¼˜åŒ–ç‰ˆ
            self.logger.info("ğŸ“„ Phase 9: å‡†å¤‡ä¼˜åŒ–çš„åˆ†æç»“æœæ•°æ®...")
            analysis_result = {
                'chain_key': chain_key, 
                'kol_name': kol_name, 
                'coin_name': coin_name,
                'chain_context': chain_context, 
                'prediction_results': prediction_results,
                'short_analysis': short_analysis, 
                'long_analysis': long_analysis,
                'final_evaluation': final_evaluation, 
                'chart_data': chart_data,
                'analysis_timestamp': datetime.now().isoformat(), 
                'version': 'optimized_for_html_generator',
                'optimization_features': {
                    'html_hotpoint_compatible': True,
                    'prediction_coordinates_ready': bool(chart_data and chart_data.get('prediction_coordinates')),
                    'tweet_quality_evaluation_complete': True,
                    'comprehensive_analysis_depth': 'professional_grade'
                }
            }
            
            # ä¿å­˜JSON
            json_path = os.path.join(self.output_dir, "kol_analysis", f"{kol_name}_{coin_name}_optimized_analysis.json")
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2, default=str)
            analysis_result['json_path'] = json_path
            
            grade = final_evaluation.get('executive_summary', {}).get('overall_grade', 'N/A')
            score = final_evaluation.get('executive_summary', {}).get('overall_score', 'N/A')
            tier = final_evaluation.get('tier', None) or final_evaluation.get('tier', 'N/A')
            
            # è®°å½•æ ¸å¿ƒè¯„ä¼°ç»“æœ
            core_thesis = final_evaluation.get('ğŸ¯ CORE_INVESTMENT_THESIS', {})
            attention_index = core_thesis.get('å€¼å¾—å…³æ³¨æŒ‡æ•°', 'N/A')
            follow_index = core_thesis.get('å€¼å¾—è·ŸéšæŒ‡æ•°', 'N/A')
            
            # è®°å½•ä¼˜åŒ–ä¿¡æ¯
            chart_info = chart_data if chart_data else {}
            prediction_count = len(chart_info.get('prediction_coordinates', []))
            
            self.logger.info(f"ğŸ‰ ä¼˜åŒ–ç‰ˆæ¨ç†é“¾åˆ†æå®Œæˆ: @{kol_name} Ã— {coin_name}")
            self.logger.info(f"  ğŸ“Š ç»¼åˆç­‰çº§: {tier}")
            self.logger.info(f"  ğŸ¯ å…³æ³¨æŒ‡æ•°: {attention_index} | è·ŸéšæŒ‡æ•°: {follow_index}")
            self.logger.info(f"  ğŸ“ˆ å›¾è¡¨ä¼˜åŒ–: {prediction_count}ä¸ªé¢„æµ‹ç‚¹ï¼Œé…åˆHTML hotpointç³»ç»Ÿ")
            
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"ä¼˜åŒ–ç‰ˆåˆ†ææ¨ç†é“¾ {chain_key} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e), 'chain_key': chain_key}
    
    async def run_optimized_analysis_pipeline(self, limit: Optional[int] = None):
        """è¿è¡Œä¼˜åŒ–çš„å®Œæ•´åˆ†ææµç¨‹ - é…åˆHTMLç”Ÿæˆå™¨"""
        try:
            self.logger.info("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆKOLåˆ†æç³»ç»Ÿï¼ˆé…åˆHTMLç”Ÿæˆå™¨ï¼‰...")
            self.logger.info("ğŸ“‹ ä¼˜åŒ–ç‰¹æ€§:")
            self.logger.info("  âœ… å»æ‰å›¾è¡¨é¢„æµ‹ç‚¹ç»˜åˆ¶ï¼Œé…åˆHTML hotpointç³»ç»Ÿ")
            self.logger.info("  âœ… ç”Ÿæˆç²¾ç¡®çš„é¢„æµ‹åæ ‡æ•°æ®ä¾›HTMLä½¿ç”¨")
            self.logger.info("  âœ… ä¿æŒæ¨æ–‡è´¨é‡æ·±åº¦è¯„ä¼°åŠŸèƒ½")
            self.logger.info("  âœ… ä¼˜åŒ–æ•°æ®ç»“æ„ä¸HTMLç”Ÿæˆå™¨å®Œç¾é…åˆ")
            self.logger.info("  âœ… ä¿®å¤æ‰€æœ‰å·²çŸ¥æŠ€æœ¯é—®é¢˜")
            
            reasoning_chains = self.load_reasoning_chains()
            
            if not reasoning_chains:
                self.logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨ç†é“¾æ•°æ®")
                return

            if limit:
                # éšæœºé€‰æ‹©ä»¥å¢åŠ å¤šæ ·æ€§ï¼Œé¿å…æ¯æ¬¡æµ‹è¯•éƒ½åˆ†æç›¸åŒçš„é“¾
                if limit < len(reasoning_chains):
                    reasoning_chains = dict(random.sample(list(reasoning_chains.items()), limit))
                else:
                    reasoning_chains = dict(list(reasoning_chains.items())[:limit])
                self.logger.info(f"ğŸ”§ é™åˆ¶åˆ†ææ•°é‡: {len(reasoning_chains)} æ¡æ¨ç†é“¾")
            
            self.logger.info(f"ğŸ“Š å¼€å§‹ä¼˜åŒ–ç‰ˆåˆ†æ {len(reasoning_chains)} æ¡æ¨ç†é“¾...")
            
            analysis_results = []
            successful_analyses, failed_analyses = 0, 0
            
            for i, (chain_key, chain) in enumerate(reasoning_chains.items(), 1):
                self.logger.info(f"\n{'='*70}\nğŸ“ˆ [{i}/{len(reasoning_chains)}] ä¼˜åŒ–ç‰ˆåˆ†æ: {chain_key}\n{'='*70}")
                try:
                    result = await self.analyze_reasoning_chain_with_html_optimization(chain_key, chain)
                    if 'error' not in result:
                        analysis_results.append(result)
                        successful_analyses += 1
                        
                        # è®°å½•ä¼˜åŒ–æ•ˆæœ
                        optimization_features = result.get('optimization_features', {})
                        chart_data = result.get('chart_data', {})
                        prediction_count = len(chart_data.get('prediction_coordinates', []))
                        
                        self.logger.info(f"  ğŸ” ä¼˜åŒ–æ•ˆæœ: HTMLå…¼å®¹{optimization_features.get('html_hotpoint_compatible', False)}, {prediction_count}ä¸ªé¢„æµ‹åæ ‡ç”Ÿæˆ")
                        
                    else:
                        failed_analyses += 1
                        self.logger.error(f"âŒ æ¨ç†é“¾ {chain_key} åˆ†æå¤±è´¥: {result.get('error')}")

                    if i < len(reasoning_chains):
                        self.logger.info("â³ çŸ­æš‚ä¼‘æ•´5ç§’ï¼Œå‡†å¤‡ä¸‹ä¸€æ¡åˆ†æ...")
                        await asyncio.sleep(5)

                except Exception as e:
                    failed_analyses += 1
                    self.logger.error(f"âŒ æ¨ç†é“¾ {chain_key} ä¼˜åŒ–ç‰ˆåˆ†ææ—¶å‘ç”Ÿæ„å¤–å¼‚å¸¸: {e}")
                    import traceback
                    traceback.print_exc()

            self.logger.info(f"\n{'='*70}\nğŸ“Š ç”ŸæˆHTMLæŠ¥å‘Š...\n{'='*70}")
            
            # ç”ŸæˆHTMLæŠ¥å‘Š - ä½¿ç”¨ä¼˜åŒ–çš„HTMLç”Ÿæˆå™¨
            # åˆ†é…ç­‰çº§ï¼ˆTierï¼‰ï¼Œå¹¶ç§»é™¤æ˜¾å¼åˆ†æ•°å­—æ®µä»¥ä¾¿HTMLåªæ˜¾ç¤ºç­‰çº§
            tier_counts = self.assign_tiers(analysis_results)
            self.logger.info(f"ğŸ”– å·²ä¸º {len(analysis_results)} ä¸ªKOLåˆ†é…ç­‰çº§: {tier_counts}")
            if self.html_generator:
                all_reports = await self.html_generator.generate_all_reports(analysis_results, self.output_dir)
                summary_report = all_reports.get('summary_report')
            else:
                summary_report = None
            
            self.logger.info(f"\nğŸ‰ ä¼˜åŒ–ç‰ˆåˆ†ææµç¨‹ç»“æŸ! æˆåŠŸ: {successful_analyses}, å¤±è´¥: {failed_analyses}")
            self.logger.info(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Š: {summary_report}")
            
            # æ€»ç»“ä¼˜åŒ–æ•ˆæœ
            self.logger.info(f"\nğŸ”§ ä¼˜åŒ–æ•ˆæœæ€»ç»“:")
            total_predictions = sum(len(r.get('prediction_results', [])) for r in analysis_results)
            total_chart_coordinates = sum(len(r.get('chart_data', {}).get('prediction_coordinates', [])) for r in analysis_results)
            total_quality_evaluations = sum(sum(1 for p in r.get('prediction_results', []) if p.get('comprehensive_analysis', {}).get('ğŸ¯ TWEET_QUALITY_DEEP_EVALUATION')) for r in analysis_results)
            html_compatible_count = sum(1 for r in analysis_results if r.get('optimization_features', {}).get('html_hotpoint_compatible', False))
            
            self.logger.info(f"  ğŸ“Š æ€»é¢„æµ‹æ•°: {total_predictions}")
            self.logger.info(f"  ğŸ“ˆ å›¾è¡¨åæ ‡ç”Ÿæˆ: {total_chart_coordinates}/{total_predictions} ({(total_chart_coordinates/total_predictions*100) if total_predictions > 0 else 0:.1f}%)")
            self.logger.info(f"  ğŸ¯ æ¨æ–‡è´¨é‡è¯„ä¼°å®Œæˆ: {total_quality_evaluations}/{total_predictions} ({(total_quality_evaluations/total_predictions*100) if total_predictions > 0 else 0:.1f}%)")
            self.logger.info(f"  ğŸŒ HTMLå…¼å®¹æ€§: {html_compatible_count}/{len(analysis_results)} ({(html_compatible_count/len(analysis_results)*100) if analysis_results else 0:.1f}%)")
            
            return {
                'successful_analyses': successful_analyses, 
                'failed_analyses': failed_analyses,
                'total_chains': len(reasoning_chains), 
                'analysis_results': analysis_results,
                'summary_report': summary_report, 
                'output_directory': self.output_dir,
                'optimization_summary': {
                    'html_hotpoint_compatible': True,
                    'prediction_coordinates_generated': total_chart_coordinates,
                    'tweet_quality_evaluations_completed': total_quality_evaluations,
                    'chart_optimization_enabled': True,
                    'all_technical_issues_fixed': True
                }
            }
        except Exception as e:
            self.logger.error(f"ä¼˜åŒ–ç‰ˆåˆ†ææµç¨‹é¡¶å±‚å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            raise

    async def run_optimized_analysis_pipeline_parallel(self, limit: Optional[int] = None):
        """å¹¶è¡Œç‰ˆæœ¬çš„åˆ†ææµç¨‹ï¼ˆé“¾çº§å¹¶å‘ï¼Œå—é™å¹¶å‘åº¦ï¼‰"""
        try:
            self.logger.info("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆKOLåˆ†æç³»ç»Ÿï¼ˆé…åˆHTMLç”Ÿæˆå™¨ï¼‰... å¹¶è¡Œé“¾åˆ†æ")
            self.logger.info("ğŸ“‹ ä¼˜åŒ–ç‰¹æ€§: é“¾çº§å¹¶å‘ + æ¨æ–‡çº§å¹¶å‘ï¼Œå—é™å¹¶å‘åº¦ä¿æŠ¤é…é¢")

            reasoning_chains = self.load_reasoning_chains()
            if not reasoning_chains:
                self.logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨ç†é“¾æ•°æ®")
                return

            if limit:
                if limit < len(reasoning_chains):
                    reasoning_chains = dict(random.sample(list(reasoning_chains.items()), limit))
                else:
                    reasoning_chains = dict(list(reasoning_chains.items())[:limit])
                self.logger.info(f"ğŸ”§ é™åˆ¶åˆ†ææ•°é‡: {len(reasoning_chains)} æ¡æ¨ç†é“¾")

            self.logger.info(f"ğŸ“Š å¼€å§‹å¹¶è¡Œåˆ†æ {len(reasoning_chains)} æ¡æ¨ç†é“¾ï¼ˆé“¾çº§å¹¶å‘ï¼‰...")

            chain_semaphore = asyncio.Semaphore(2)

            async def analyze_single_chain(chain_key: str, chain: List[Dict]):
                async with chain_semaphore:
                    try:
                        return await self.analyze_reasoning_chain_with_html_optimization(chain_key, chain)
                    except Exception as e:
                        self.logger.error(f"âŒ æ¨ç†é“¾ {chain_key} å¹¶è¡Œåˆ†æå¤±è´¥: {e}")
                        return {'error': str(e), 'chain_key': chain_key}

            tasks = [asyncio.create_task(analyze_single_chain(k, v)) for k, v in reasoning_chains.items()]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            analysis_results = [r for r in results if isinstance(r, dict) and 'error' not in r]
            failed_analyses = len([r for r in results if isinstance(r, dict) and 'error' in r])

            self.logger.info(f"\n{'='*70}\nğŸ“Š ç”ŸæˆHTMLæŠ¥å‘Š...\n{'='*70}")
            # åˆ†é…ç­‰çº§ï¼ˆTierï¼‰ï¼Œå¹¶ç§»é™¤æ˜¾å¼åˆ†æ•°å­—æ®µä»¥ä¾¿HTMLåªæ˜¾ç¤ºç­‰çº§
            tier_counts = self.assign_tiers(analysis_results)
            self.logger.info(f"ğŸ”– å·²ä¸º {len(analysis_results)} ä¸ªKOLåˆ†é…ç­‰çº§: {tier_counts}")
            if self.html_generator:
                all_reports = await self.html_generator.generate_all_reports(analysis_results, self.output_dir)
                summary_report = all_reports.get('summary_report')
            else:
                summary_report = None

            self.logger.info(f"\nğŸ‰ å¹¶è¡Œç‰ˆåˆ†ææµç¨‹ç»“æŸ! æˆåŠŸ: {len(analysis_results)}, å¤±è´¥: {failed_analyses}")
            self.logger.info(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Š: {summary_report}")

            total_predictions = sum(len(r.get('prediction_results', [])) for r in analysis_results)
            total_chart_coordinates = sum(len(r.get('chart_data', {}).get('prediction_coordinates', [])) for r in analysis_results)
            total_quality_evaluations = sum(
                sum(1 for p in r.get('prediction_results', []) if p.get('comprehensive_analysis', {}).get('ğŸ¯ TWEET_QUALITY_DEEP_EVALUATION'))
                for r in analysis_results
            )
            html_compatible_count = sum(1 for r in analysis_results if r.get('optimization_features', {}).get('html_hotpoint_compatible', False))

            self.logger.info(f"  ğŸ“Š æ€»é¢„æµ‹æ•°: {total_predictions}")
            self.logger.info(f"  ğŸ“ˆ å›¾è¡¨åæ ‡ç”Ÿæˆ: {total_chart_coordinates}/{total_predictions} ({(total_chart_coordinates/total_predictions*100) if total_predictions > 0 else 0:.1f}%)")
            self.logger.info(f"  ğŸ¯ æ¨æ–‡è´¨é‡è¯„ä¼°å®Œæˆ: {total_quality_evaluations}/{total_predictions} ({(total_quality_evaluations/total_predictions*100) if total_predictions > 0 else 0:.1f}%)")
            self.logger.info(f"  ğŸŒ HTMLå…¼å®¹æ€§: {html_compatible_count}/{len(analysis_results)} ({(html_compatible_count/len(analysis_results)*100) if analysis_results else 0:.1f}%)")

            return {
                'successful_analyses': len(analysis_results),
                'failed_analyses': failed_analyses,
                'total_chains': len(reasoning_chains),
                'analysis_results': analysis_results,
                'summary_report': summary_report,
                'output_directory': self.output_dir,
                'optimization_summary': {
                    'html_hotpoint_compatible': True,
                    'prediction_coordinates_generated': total_chart_coordinates,
                    'tweet_quality_evaluations_completed': total_quality_evaluations,
                    'chart_optimization_enabled': True,
                    'all_technical_issues_fixed': True
                }
            }
        except Exception as e:
            self.logger.error(f"å¹¶è¡Œç‰ˆåˆ†ææµç¨‹é¡¶å±‚å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            raise

# ========================================================================
# å‘½ä»¤è¡Œå‚æ•°è§£æå’Œä¸»å‡½æ•° - ä¼˜åŒ–ç‰ˆ
# ========================================================================

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='ä¼˜åŒ–ç‰ˆKOLåˆ†æå™¨V2 - é…åˆHTMLç”Ÿæˆå™¨',
        formatter_class=argparse.RawTextHelpFormatter,
        epilog='''
ä¼˜åŒ–ç‰¹æ€§:
  âœ… å»æ‰å›¾è¡¨é¢„æµ‹ç‚¹ç»˜åˆ¶ï¼Œé…åˆHTML hotpointç³»ç»Ÿ
  âœ… ç”Ÿæˆç²¾ç¡®çš„é¢„æµ‹åæ ‡æ•°æ®ä¾›HTMLä½¿ç”¨
  âœ… ä¿æŒæ¨æ–‡è´¨é‡æ·±åº¦è¯„ä¼°åŠŸèƒ½
  âœ… ä¼˜åŒ–æ•°æ®ç»“æ„ä¸HTMLç”Ÿæˆå™¨å®Œç¾é…åˆ
  âœ… ä¿®å¤æ‰€æœ‰å·²çŸ¥æŠ€æœ¯é—®é¢˜

ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ä¼˜åŒ–åˆ†æï¼Œé™åˆ¶ä¸º3æ¡é“¾ï¼ˆæµ‹è¯•ä¼˜åŒ–æ•ˆæœï¼‰
  python optimized_v2.py --api_key "your_openai_api_key" --limit 3

  # ä½¿ç”¨CoinGecko Pro APIè¿›è¡Œå®Œæ•´ä¼˜åŒ–åˆ†æ
  python optimized_v2.py --api_key "your_openai_api_key" --coingecko_api_key "your_coingecko_pro_key"

  # è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆæŸ¥çœ‹ä¼˜åŒ–æ•ˆæœï¼‰
  python optimized_v2.py --api_key "your_openai_api_key" --verbose
        '''
    )
    
    parser.add_argument('--db_dir', type=str, default='./data', 
                       help='æ•°æ®åº“ç›®å½•è·¯å¾„ (é»˜è®¤: ./data)')
    parser.add_argument('--api_key', type=str, required=True, 
                       help='OpenAI APIå¯†é’¥ (å¿…éœ€)')
    parser.add_argument('--coingecko_api_key', type=str, default="ä½ çš„api key", 
                       help='CoinGecko Pro APIå¯†é’¥ (å¯é€‰ï¼Œå»ºè®®ä½¿ç”¨ä»¥è·å¾—æ›´å¥½çš„åˆ†ææ•ˆæœ)')
    parser.add_argument('--limit', type=int, default=None, 
                       help='é™åˆ¶åˆ†æçš„æ¨ç†é“¾æ•°é‡ (å¯é€‰ï¼Œç”¨äºæµ‹è¯•)')
    parser.add_argument('--verbose', action='store_true', 
                       help='å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º')
    parser.add_argument('--parallel_chains', action='store_true', 
                       help='å¯ç”¨é“¾çº§å¹¶è¡Œåˆ†æï¼ˆä¸æ¨æ–‡çº§å¹¶å‘å åŠ ï¼Œå»ºè®®æ§åˆ¶æ€»å¹¶å‘ï¼‰')
    
    return parser.parse_args()

async def main():
    """ä¸»å‡½æ•° - ä¼˜åŒ–ç‰ˆKOLåˆ†æå™¨å…¥å£ï¼ˆé…åˆHTMLç”Ÿæˆå™¨ï¼‰"""
    try:
        args = parse_arguments()
        
        # æ ¹æ®--verboseå‚æ•°è®¾ç½®æ—¥å¿—çº§åˆ«
        if args.verbose:
            # ç¡®ä¿æ—¥å¿—è®°å½•å™¨å·²åˆ›å»º
            if not logging.getLogger().hasHandlers():
                logging.basicConfig(level=logging.DEBUG)
            else:
                logging.getLogger().setLevel(logging.DEBUG)
        
        print("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆKOLåˆ†æå™¨V2 - é…åˆHTMLç”Ÿæˆå™¨")
        print("=" * 80)
        print("ğŸ”§ ä¸»è¦ä¼˜åŒ–ç‰¹æ€§:")
        print("   âœ… å»æ‰å›¾è¡¨é¢„æµ‹ç‚¹ç»˜åˆ¶ï¼Œé…åˆHTML hotpointç³»ç»Ÿ")
        print("   âœ… ç”Ÿæˆç²¾ç¡®çš„é¢„æµ‹åæ ‡æ•°æ®ä¾›HTMLä½¿ç”¨")
        print("   âœ… ä¿æŒæ¨æ–‡è´¨é‡æ·±åº¦è¯„ä¼°åŠŸèƒ½")
        print("   âœ… ä¼˜åŒ–æ•°æ®ç»“æ„ä¸HTMLç”Ÿæˆå™¨å®Œç¾é…åˆ")
        print("   âœ… ä¿®å¤æ‰€æœ‰å·²çŸ¥æŠ€æœ¯é—®é¢˜ï¼ˆ422é”™è¯¯ã€æ—¶é—´æˆ³ã€æˆªæ–­ç­‰ï¼‰")
        print("   ğŸ¯ æ ¸å¿ƒç‰¹æ€§ï¼šæ¨æ–‡è´¨é‡æ·±åº¦è¯„ä¼°")
        print("      - å›ç­”'è¿™æ¡æ¨æ–‡åˆ°åº•æ˜¯å¥½è¿˜æ˜¯åï¼Ÿ'")
        print("      - æä¾›è¯¦ç»†çš„æŠ•èµ„ä»·å€¼åˆ†æ")
        print("      - ç»™å‡ºå…·ä½“çš„è·Ÿéšå»ºè®®å’Œé£é™©è­¦ç¤º")
        print("      - å®Œç¾é…åˆHTMLäº¤äº’å¼hotpointç³»ç»Ÿ")
        print("=" * 80)
        print(f"ğŸ“ æ•°æ®åº“ç›®å½•: {args.db_dir}")
        print(f"ğŸ”‘ OpenAI API: {'å·²é…ç½®' if args.api_key else 'æœªé…ç½®'}")
        print(f"ğŸ”‘ CoinGecko API: {'Proç‰ˆæœ¬' if args.coingecko_api_key else 'å…è´¹ç‰ˆæœ¬'}")
        if args.limit:
            print(f"ğŸ”§ åˆ†æé™åˆ¶: {args.limit} æ¡æ¨ç†é“¾")
        print("=" * 80)
        print("ğŸ¯ åˆ†æç‰¹è‰²ï¼ˆä¼˜åŒ–ç‰ˆï¼‰:")
        print("   â€¢ æ·±åº¦AIåˆ†æå’Œé¢„æµ‹é€»è¾‘æŒ–æ˜")
        print("   â€¢ çœŸå®å†å²ä»·æ ¼éªŒè¯ï¼ˆä¿®å¤422é”™è¯¯ï¼‰")
        print("   â€¢ ç»¼åˆæœç´¢ç»“æœåˆ†æï¼ˆå®Œæ•´å±•ç¤ºï¼Œä¸æˆªæ–­ï¼‰") 
        print("   â€¢ æ¨æ–‡è´¨é‡æ·±åº¦è¯„ä¼°ï¼ˆæ–°å¢æ ¸å¿ƒåŠŸèƒ½ï¼‰")
        print("   â€¢ ä¸“ä¸šçº§æŠ•èµ„å»ºè®®å’Œé£é™©æ§åˆ¶æŒ‡å¯¼")
        print("   â€¢ æœºæ„çº§è¯„ä¼°æŠ¥å‘Š")
        print("   â€¢ ä¼˜åŒ–å›¾è¡¨ç”Ÿæˆï¼Œé…åˆHTML hotpointäº¤äº’ç³»ç»Ÿ")
        print("=" * 80)
        
        # éªŒè¯è¾“å…¥
        if not os.path.exists(args.db_dir):
            print(f"âŒ é”™è¯¯: æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨: {args.db_dir}")
            return
        
        crypto_db_path = os.path.join(args.db_dir, "crypto_recommendations.db")
        if not os.path.exists(crypto_db_path):
            print(f"âŒ é”™è¯¯: æ¨èæ•°æ®åº“ä¸å­˜åœ¨: {crypto_db_path}")
            return
        
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = EnhancedKOLAnalyzerV2(
            db_dir=args.db_dir,
            api_key=args.api_key,
            coingecko_api_key=args.coingecko_api_key
        )
        
        print("\nğŸ”¥ å¼€å§‹ä¼˜åŒ–ç‰ˆåˆ†ææµç¨‹ï¼ˆé…åˆHTMLç”Ÿæˆå™¨ï¼‰...")
        print("ğŸ“‹ åˆ†æé˜¶æ®µ:")
        print("   1ï¸âƒ£ æ¨ç†é“¾èƒŒæ™¯æ·±åº¦é¢„å¤„ç†")
        print("   2ï¸âƒ£ ä¸“ä¸šçº§æ¨æ–‡å†…å®¹åˆ†æï¼ˆå«è´¨é‡è¯„ä¼°ï¼‰") 
        print("   3ï¸âƒ£ æ·±åº¦æœç´¢å’Œæ•°æ®æ”¶é›†ï¼ˆä¿®å¤APIé”™è¯¯ï¼‰")
        print("   4ï¸âƒ£ çœŸå®ä»·æ ¼éªŒè¯ï¼ˆä¿®å¤æ—¶é—´æˆ³é—®é¢˜ï¼‰")
        print("   5ï¸âƒ£ ç»¼åˆè¯„ä¼°å’Œæ´å¯Ÿï¼ˆå›ç­”æ¨æ–‡å¥½åï¼‰")
        print("   6ï¸âƒ£ ä¼˜åŒ–å›¾è¡¨ç”Ÿæˆï¼ˆé…åˆHTML hotpointç³»ç»Ÿï¼‰")
        print("   7ï¸âƒ£ ä¸“ä¸šçº§HTMLæŠ¥å‘Šç”Ÿæˆ")
        print()
        
        # æ‰§è¡Œåˆ†æ
        results = await analyzer.run_optimized_analysis_pipeline_parallel(limit=args.limit)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ ä¼˜åŒ–ç‰ˆåˆ†æå®Œæˆï¼ˆé…åˆHTMLç”Ÿæˆå™¨ï¼‰!")
        print("=" * 80)
        
        if results and 'successful_analyses' in results:
            print(f"âœ… æˆåŠŸåˆ†æ: {results.get('successful_analyses', 0)} æ¡æ¨ç†é“¾")
            print(f"âŒ å¤±è´¥åˆ†æ: {results.get('failed_analyses', 0)} æ¡æ¨ç†é“¾")
            total_chains = results.get('total_chains', 1)
            if total_chains > 0:
                success_rate = results.get('successful_analyses', 0) / total_chains * 100
                print(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Š: {results.get('summary_report', 'æœªç”Ÿæˆ')}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {results.get('output_directory', 'N/A')}")
            
            # æ˜¾ç¤ºä¼˜åŒ–æ•ˆæœ
            optimization_summary = results.get('optimization_summary', {})
            print("\nğŸ”§ ä¼˜åŒ–æ•ˆæœéªŒè¯:")
            print(f"   âœ… HTML hotpointå…¼å®¹: {optimization_summary.get('html_hotpoint_compatible', False)}")
            print(f"   ğŸ“ˆ é¢„æµ‹åæ ‡ç”Ÿæˆ: {optimization_summary.get('prediction_coordinates_generated', 0)}ä¸ª")
            print(f"   ğŸ¯ æ¨æ–‡è´¨é‡è¯„ä¼°: {optimization_summary.get('tweet_quality_evaluations_completed', 0)}ä¸ª")
            print(f"   ğŸ“Š å›¾è¡¨ä¼˜åŒ–å¯ç”¨: {optimization_summary.get('chart_optimization_enabled', False)}")
            print(f"   ğŸ› ï¸ æŠ€æœ¯é—®é¢˜ä¿®å¤: {optimization_summary.get('all_technical_issues_fixed', False)}")
            
            print("\nğŸ¯ åˆ†æäº®ç‚¹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰:")
            print("   â€¢ æ¯ä¸ªé¢„æµ‹éƒ½è¿›è¡Œäº†å¤šç»´åº¦æ·±åº¦åˆ†æ")
            print("   â€¢ åŸºäºçœŸå®å†å²ä»·æ ¼çš„ç²¾ç¡®éªŒè¯ï¼ˆä¿®å¤APIé”™è¯¯ï¼‰")
            print("   â€¢ æ¨æ–‡è´¨é‡æ·±åº¦è¯„ä¼° - æ˜ç¡®å›ç­”'å¥½'è¿˜æ˜¯'å'")
            print("   â€¢ ä¸“ä¸šçº§æŠ•èµ„å»ºè®®å’Œé£é™©è¯„ä¼°")
            print("   â€¢ æœºæ„çº§åˆ«çš„KOLè¯„ä¼°æ ‡å‡†")
            print("   â€¢ ä¼˜åŒ–çš„å›¾è¡¨ç³»ç»Ÿï¼Œå®Œç¾é…åˆHTML hotpointäº¤äº’")
            print("   â€¢ è¯¦ç»†çš„HTMLæŠ¥å‘Šæ”¯æŒäº¤äº’å¼æŸ¥çœ‹")
            
            # æ˜¾ç¤ºæœ€ä½³è¡¨ç°çš„KOLï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            analysis_results = results.get('analysis_results', [])
            if analysis_results:
                # é€‰æ‹©å†…éƒ¨è¯„åˆ†æœ€é«˜çš„ä½œä¸ºæ ·ä¾‹ï¼ˆå¦‚æœæ²¡æœ‰_internal_scoreåˆ™é€€å›ä¸º0ï¼‰
                best_result = max(
                    analysis_results,
                    key=lambda x: x.get('final_evaluation', {}).get('_internal_score', 0)
                )
                core_thesis = best_result.get('final_evaluation', {}).get('ğŸ¯ CORE_INVESTMENT_THESIS', {})
                tier = best_result.get('final_evaluation', {}).get('tier', 'N/A')
                
                print("\nğŸ† æœ€ä½³è¡¨ç°KOL:")
                print(f"   @{best_result.get('kol_name', 'N/A')} Ã— {best_result.get('coin_name', 'N/A')}")
                print(f"   ç­‰çº§: {tier}")
                if core_thesis:
                    print(f"   å…³æ³¨æŒ‡æ•°: {core_thesis.get('å€¼å¾—å…³æ³¨æŒ‡æ•°', 'N/A')}")
                    print(f"   è·ŸéšæŒ‡æ•°: {core_thesis.get('å€¼å¾—è·ŸéšæŒ‡æ•°', 'N/A')}")
                    print(f"   é£é™©çº§åˆ«: {core_thesis.get('é£é™©è­¦ç¤ºçº§åˆ«', 'N/A')}")
                
                # æ˜¾ç¤ºä¼˜åŒ–æ•ˆæœ
                optimization_features = best_result.get('optimization_features', {})
                chart_data = best_result.get('chart_data', {})
                if chart_data:
                    prediction_count = len(chart_data.get('prediction_coordinates', []))
                    chart_version = chart_data.get('version', 'N/A')
                    print(f"\nğŸ“Š å›¾è¡¨ä¼˜åŒ–æ ·ä¾‹:")
                    print(f"   ç‰ˆæœ¬: {chart_version}")
                    print(f"   é¢„æµ‹åæ ‡: {prediction_count}ä¸ª")
                    print(f"   HTMLå…¼å®¹: {optimization_features.get('html_hotpoint_compatible', False)}")
                    print(f"   åæ ‡å°±ç»ª: {optimization_features.get('prediction_coordinates_ready', False)}")
                
                # æ˜¾ç¤ºæ¨æ–‡è´¨é‡è¯„ä¼°æ•ˆæœ
                prediction_results = best_result.get('prediction_results', [])
                quality_evaluations = [p for p in prediction_results if p.get('comprehensive_analysis', {}).get('ğŸ¯ TWEET_QUALITY_DEEP_EVALUATION')]
                if quality_evaluations:
                    print(f"\nğŸ“Š æ¨æ–‡è´¨é‡è¯„ä¼°æ ·ä¾‹ï¼ˆå…±{len(quality_evaluations)}æ¡ï¼‰:")
                    sample_eval = quality_evaluations[0].get('comprehensive_analysis', {}).get('ğŸ¯ TWEET_QUALITY_DEEP_EVALUATION', {})
                    if sample_eval:
                        print(f"   æ¨æ–‡è´¨é‡åˆ¤æ–­: {sample_eval.get('æ¨æ–‡è´¨é‡åˆ¤æ–­', 'N/A')}")
                        print(f"   ç»¼åˆè¯„åˆ†: {sample_eval.get('ç»¼åˆè¯„åˆ†', 'N/A')}")
                        print(f"   å†…å®¹è´¨é‡: {sample_eval.get('content_quality_score', 'N/A')}")
                        print(f"   é¢„æµ‹ä»·å€¼: {sample_eval.get('prediction_value_score', 'N/A')}")
                        print(f"   KOLè´£ä»»: {sample_eval.get('kol_responsibility_score', 'N/A')}")
                
        else:
            print("âŒ åˆ†ææµç¨‹æœªè¿”å›æœ‰æ•ˆç»“æœæˆ–ä¸­é€”å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—ã€‚")
        
        print("\nğŸ’¡ æç¤º: æŸ¥çœ‹ç”Ÿæˆçš„HTMLæŠ¥å‘Šè·å–å®Œæ•´çš„ä¸“ä¸šåˆ†æç»“æœ")
        print("ğŸ”§ ä¼˜åŒ–è¯´æ˜: æœ¬ç‰ˆæœ¬ä¸“é—¨ä¼˜åŒ–é…åˆHTMLç”Ÿæˆå™¨ï¼Œå›¾è¡¨ä¸hotpointç³»ç»Ÿå®Œç¾ç»“åˆ")
        print("ğŸ¯ æ ¸å¿ƒç‰¹æ€§: æ¨æ–‡è´¨é‡æ·±åº¦è¯„ä¼°åŠŸèƒ½å¸®åŠ©ç”¨æˆ·æ˜ç¡®åˆ¤æ–­KOLæ¨æ–‡ä»·å€¼")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­äº†åˆ†æè¿‡ç¨‹")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºé¡¶å±‚æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
        print("   1. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®ä¸”æœ‰è¶³å¤Ÿé¢åº¦")
        print("   2. ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œä¸”å¯ä»¥è®¿é—®OpenAIå’ŒCoinGecko API")
        print("   3. éªŒè¯æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å¯è®¿é—®")
        print("   4. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        print("   5. æœ¬ä¼˜åŒ–ç‰ˆæœ¬å·²è§£å†³äº†å¤§éƒ¨åˆ†å·²çŸ¥æŠ€æœ¯é—®é¢˜")
        print("   6. ç¡®è®¤HTMLç”Ÿæˆå™¨æ–‡ä»¶å­˜åœ¨ä¸”å¯æ­£å¸¸å¯¼å…¥")

if __name__ == "__main__":
    # è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥ï¼ˆWindowså…¼å®¹æ€§ï¼‰
    try:
        import asyncio
        if hasattr(asyncio, 'WindowsProactorEventLoopPolicy') and os.name == 'nt':
            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    except:
        pass
    
    asyncio.run(main())
